{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "809f0aef-17eb-4d80-bedc-fe0fd6513ff4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Machine learning at CoDaS-HEP 2024, lesson 1 part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938d2f75-d4f5-427e-83a1-cf892eb47412",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7915de53-a128-495e-8e52-60b1dd26383d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Reminders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b62d6ff-0621-49a4-9eab-8b986f1703e7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "As a reminder, a (fully-connected, feed-forward, 4-layer) neural network looks like this:\n",
    "\n",
    "<img src=\"../img/artificial-neural-network-layers-2.svg\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a50e9b-d56d-4240-aeb8-cd9a1f9527ad",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31d4d6f-30f9-4d2f-aa2d-4d5dfc685d50",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Which is to say, like this:\n",
    "\n",
    "$$\n",
    "y_i =\n",
    "f\\left(a_{i,j}^{\\mbox{\\scriptsize L3-L4}} \\cdot\n",
    "f\\left(a_{i,j}^{\\mbox{\\scriptsize L2-L3}} \\cdot\n",
    "f\\left(a_{i,j}^{\\mbox{\\scriptsize L1-L2}} \\cdot x_j + b_i^{\\mbox{\\scriptsize L1-L2}}\\right)\n",
    "+ b_i^{\\mbox{\\scriptsize L2-L3}}\\right)\n",
    "+ b_i^{\\mbox{\\scriptsize L3-L4}}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4ac88c-2816-45d7-8bff-ad4fe386dbc0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05902288-0f05-44b7-9a87-9be7dc56fc5a",
   "metadata": {},
   "source": [
    "In code, it could be implemented as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5526b2-4026-4554-a2a9-a9fc5b542605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b74fec-4ad8-4f16-83b8-d3d874e0c44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take 8-dimensional input layer 1 to 7-dimensional hidden layer 2\n",
    "a_L1_L2 = np.random.normal(0, 1, (7, 8))\n",
    "b_L1_L2 = np.random.normal(0, 1, (7,))\n",
    "\n",
    "# take 7-dimensional hidden layer 2 to 9-dimensional hidden layer 3\n",
    "a_L2_L3 = np.random.normal(0, 1, (9, 7))\n",
    "b_L2_L3 = np.random.normal(0, 1, (9,))\n",
    "\n",
    "# take 9-dimensional hidden layer 3 to 6-dimensional output layer 4\n",
    "a_L3_L4 = np.random.normal(0, 1, (6, 9))\n",
    "b_L3_L4 = np.random.normal(0, 1, (6,))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def model(x):\n",
    "    layer1 = x\n",
    "    layer2 = relu(a_L1_L2 @ layer1 + b_L1_L2)\n",
    "    layer3 = relu(a_L2_L3 @ layer2 + b_L2_L3)\n",
    "    layer4 = relu(a_L3_L4 @ layer3 + b_L3_L4)\n",
    "    y = layer4\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04b3ded-14f3-4719-92ca-c26a7d7d07ef",
   "metadata": {},
   "source": [
    "Here's the model's output for a sample input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdbd3d3-7587-47c0-b9a7-8b7b6d375a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.normal(0, 1, (8,))\n",
    "\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932537b7-b575-49b9-bf68-946603778091",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778f21dc-bf6a-436f-86ce-3a6d42449618",
   "metadata": {},
   "source": [
    "Given a large dataset of `x` vectors, an equally large set of expected `y` vectors, and a minimizer, we could train the model by optimizing these parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d112a58-cf42-40a5-a86a-ab7164e67272",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_L1_L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9072e6fd-a55c-489f-85b6-c41e796a8456",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_L1_L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6917d2-1602-44ba-8ec7-726ac84ce4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_L2_L3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78869c8a-e585-440a-ab79-9724f8be9a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_L2_L3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308414ec-6e53-407d-b73e-5a0177ddebda",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_L3_L4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bedc6b7-82d5-4029-ba39-5587b2723873",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_L3_L4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f7330b-5431-47ec-8567-e9981d440680",
   "metadata": {},
   "source": [
    "such that `model(x)` comes as close as possible to `y`.\n",
    "\n",
    "Then we could use `model(x_new)` to predict new $y$ values for `x_new`, and the predictions would have (roughly) the same correlations as the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee950c0-7000-4a1a-bf2b-4ff9f07377e0",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8c312e-dd9c-4e30-a98c-579e45bbab4c",
   "metadata": {},
   "source": [
    "HEP has a favorite minimizer: MINUIT.\n",
    "\n",
    "Introduced in 1972 by Fred James, MINUIT computes numerical second derivatives of the function, attempts to jump to the minimum, and then recomputes.\n",
    "\n",
    "<img src=\"../img/minuit-1975.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ca2f27-5e50-4942-a11e-f16f73fb5b6a",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1104e788-0c24-4913-8fac-9c9363837c1e",
   "metadata": {},
   "source": [
    "It doesn't scale well with a large number of parameters to optimize, and we would have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9112ed83-37d6-456d-8886-87765b906b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_L1_L2.size + b_L1_L2.size + a_L2_L3.size + b_L2_L3.size + a_L3_L4.size + b_L3_L4.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3d8a45-f62c-452c-b6c2-496775534912",
   "metadata": {},
   "source": [
    "parameters to optimize in this simple example.\n",
    "\n",
    "Nevertheless, I'll use MINUIT for some early examples, through the excellent iminuit package.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/scikit-hep/iminuit/develop/doc/_static/iminuit_logo.svg\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35835d0d-5e3e-4e31-8952-2c8eec905867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import iminuit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fada7909-a20d-4d91-b00d-a31f610e2718",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c319ee3-9090-4b17-84a7-6c0ba2047993",
   "metadata": {},
   "source": [
    "As another simplification, note that we don't have to maintain the distinction between matrices of parameters $a_{i,j}$ and vectors of parameters $b_i$:\n",
    "\n",
    "$$\n",
    "\\left(\\begin{array}{c c c c}\n",
    "a_{1,1} & a_{1,2} & \\ldots & a_{1,10} \\\\\n",
    "a_{2,1} & a_{2,2} & \\ldots & a_{2,10} \\\\\n",
    "a_{3,1} & a_{3,2} & \\ldots & a_{3,10} \\\\\n",
    "a_{4,1} & a_{4,2} & \\ldots & a_{4,10} \\\\\n",
    "a_{5,1} & a_{5,2} & \\ldots & a_{5,10} \\\\\n",
    "\\end{array}\\right) \\cdot \\left(\\begin{array}{c}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "\\vdots \\\\\n",
    "x_{10} \\\\\n",
    "\\end{array}\\right) + \\left(\\begin{array}{c}\n",
    "b_1 \\\\\n",
    "b_2 \\\\\n",
    "b_3 \\\\\n",
    "b_4 \\\\\n",
    "b_5 \\\\\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "is the same as\n",
    "\n",
    "$$\n",
    "\\left(\\begin{array}{c c c c c}\n",
    "a_{1,1} & a_{1,2} & \\ldots & a_{1,10} & b_1 \\\\\n",
    "a_{2,1} & a_{2,2} & \\ldots & a_{2,10} & b_2 \\\\\n",
    "a_{3,1} & a_{3,2} & \\ldots & a_{3,10} & b_3 \\\\\n",
    "a_{4,1} & a_{4,2} & \\ldots & a_{4,10} & b_4 \\\\\n",
    "a_{5,1} & a_{5,2} & \\ldots & a_{5,10} & b_5 \\\\\n",
    "\\end{array}\\right) \\cdot \\left(\\begin{array}{c}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "\\vdots \\\\\n",
    "x_{10} \\\\\n",
    "1 \\\\\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "We can absorb our $b_i$ vectors into a bigger matrix $A_{i,j}$ with the understanding that we concatenate a $1$ at the end of the $x_j$ vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bec153-f89a-48a3-ac0d-89bcd73f6a64",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987fab6f-e97c-4ec1-a949-b4a14571902b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## What's so special about this linear-nonlinear sandwich?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f72fc7-f457-40ec-987d-fc99b485518f",
   "metadata": {},
   "source": [
    "The goal of curve-fitting is to _approximate a function_ from noisy samples.\n",
    "\n",
    "Neural networks are special because they are exceptionally good at approximating functions, a fact that is formally expressed as the [universal approximation theorem(s)](https://en.wikipedia.org/wiki/Universal_approximation_theorem)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2518d2-e82e-4e64-9cb0-085815da0270",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3ec1a9-4025-478d-a07e-320a683e00fc",
   "metadata": {},
   "source": [
    "As a physicist, I've approximated a few functions in my time. What's \"exceptionally good\" about this method?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4dce70-02f3-493e-9a77-c203a5639776",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d047fb45-df7d-4f9b-bd6c-ba820fd3f3e8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Demonstrate with a sample problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaf021b-a710-452a-9525-d5ec9228b148",
   "metadata": {},
   "source": [
    "Suppose $x$ and $y$ are related as\n",
    "\n",
    "$$ y = \\left\\{\\begin{array}{l l}\n",
    "\\sin(22 x) & \\mbox{if } |x - 0.43| < 0.15 \\\\\n",
    "-1 + 3.5 x - 2 x^2 & \\mbox{otherwise} \\\\\n",
    "\\end{array}\\right. $$\n",
    "\n",
    "with small errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfb6313-b007-4de3-81e2-d74233c8c140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truth(x):\n",
    "    return np.where(abs(x - 0.43) < 0.15, np.sin(22*x), -1 + 3.5*x - 2*x**2)\n",
    "\n",
    "x = np.random.uniform(0, 1, 1000)\n",
    "y = truth(x) + np.random.normal(0, 0.03, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe66262-c1fe-4467-9882-28fb155afc07",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fbfb75-ead0-4d67-9f57-ca7b09dca8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f06485-eed6-461a-8e48-f36b523dcdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(x, y, marker=\".\")\n",
    "ax.plot(np.linspace(0, 1, 1000), truth(np.linspace(0, 1, 1000)), color=\"magenta\", linewidth=3)\n",
    "\n",
    "ax.legend([\"measurements\", \"truth\"])\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5a493b-84b2-4e83-883e-b87e349826e6",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f83a7ea-9d7d-4200-9c05-3c508327202f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Attempt 1: a linear fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169b1b75-6029-41af-967d-cc613377a6db",
   "metadata": {},
   "source": [
    "A linear fit is terrible because the curve isn't close to being linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc5b634-9360-4b3a-b086-bdf3a39cab06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a linear fit can be computed analytically, which is nice\n",
    "sum1 = len(x)\n",
    "sumx = np.sum(x)\n",
    "sumy = np.sum(y)\n",
    "sumxx = np.sum(x**2)\n",
    "sumxy = np.sum(x * y)\n",
    "delta = (sum1 * sumxx) - (sumx * sumx)\n",
    "\n",
    "slope = ((sum1 * sumxy) - (sumx * sumy)) / delta\n",
    "intercept = ((sumxx * sumy) - (sumx * sumxy)) / delta\n",
    "\n",
    "model_x = np.linspace(0, 1, 1000)\n",
    "model_y = slope * model_x + intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1ec844-fe84-40d7-a290-4eb99708d489",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(x, y, marker=\".\")\n",
    "ax.plot(np.linspace(0, 1, 1000), truth(np.linspace(0, 1, 1000)), color=\"magenta\", linewidth=3)\n",
    "ax.plot(model_x, model_y, color=\"orange\", linewidth=3)\n",
    "\n",
    "ax.legend([\"measurements\", \"truth\", \"linear fit\"])\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2298957b-3cf8-44f2-a31a-1b6a10dc8262",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816519e0-4b10-451c-bf9b-2632de5e5a7f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Attempt 2: a theory-driven ansatz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5659773a-a028-40ad-969b-2b5645a9c54e",
   "metadata": {},
   "source": [
    "A physicist's usual strategy is to find the underlying theory, with configurable parameters for the unknown values.\n",
    "\n",
    "This parameterizable function is called an \"ansatz\".\n",
    "\n",
    "Suppose we _just know_ that the functional form is\n",
    "\n",
    "$$ y = \\left\\{\\begin{array}{l l}\n",
    "\\sin(C x) & \\mbox{if } |x - A| < B \\\\\n",
    "D + E x + F x^2 & \\mbox{otherwise} \\\\\n",
    "\\end{array}\\right. $$\n",
    "\n",
    "for some $A$, $B$, $C$, $D$, $E$, $F$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b314ad-d173-4427-9a93-0b5a5a87ec53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ansatz(x, A, B, C, D, E, F):\n",
    "    return np.where(abs(x - A) < B, np.sin(C*x), D + E*x + F*x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c44a5e-2c7c-48ea-b863-71be2fd1407e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from iminuit.cost import LeastSquares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a535702-0ba5-4087-9bf4-b894a7c3fcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a loss function that is minimized when the parameterized ansatz is equal to truth\n",
    "least_squares = LeastSquares(x, y, 0.03, ansatz)\n",
    "\n",
    "# set initial parameter values\n",
    "minimizer = iminuit.Minuit(least_squares, A=0.43, B=0.15, C=22, D=-1, E=3.5, F=-2)\n",
    "minimizer.migrad()\n",
    "\n",
    "model_x = np.linspace(0, 1, 1000)\n",
    "model_y = ansatz(model_x, **{p.name: p.value for p in minimizer.params})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cde7986-0ca1-47cb-ae98-9bd4b3949407",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(x, y, marker=\".\")\n",
    "ax.plot(np.linspace(0, 1, 1000), truth(np.linspace(0, 1, 1000)), color=\"magenta\", linewidth=3)\n",
    "ax.plot(model_x, model_y, color=\"orange\", linewidth=3)\n",
    "\n",
    "ax.legend([\"measurements\", \"truth\", \"ansatz fit\"])\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4f645c-78b1-4e29-b196-d5d922266c97",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203cbc8c-e5d4-4aaa-88bf-49ca8ba04092",
   "metadata": {},
   "source": [
    "It's a great fit, but\n",
    "\n",
    "* what if we don't know the functional form? or if it's super-complicated, like human behavior?\n",
    "* the fit depends sensitively on the initial parameters and step size (try starting any of the parameters at the wrong value).\n",
    "\n",
    "Minuit was designed as an interactive program so that users could hand-tweak their fits.\n",
    "\n",
    "How many of you ever had to tinker with a fit until it converged?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ca13c2-e03e-4e1f-a846-c763f61bcab9",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b74d92-8117-47f3-b3d4-d3c48af0fbf8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Attempt 3: orthonormal basis functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37848ca4-c254-4a51-bece-e5247c0daa5c",
   "metadata": {},
   "source": [
    "As physicists, we would approach a _generic_ unknown function with a [Taylor series](https://en.wikipedia.org/wiki/Taylor_series), a [Fourier series](https://en.wikipedia.org/wiki/Fourier_series), or other sum of orthonormal basis functions ([Jacobi](https://en.wikipedia.org/wiki/Jacobi_polynomials), [Laguerre](https://en.wikipedia.org/wiki/Laguerre_polynomials), [Hermite](https://en.wikipedia.org/wiki/Hermite_polynomials), [Chebyshev](https://en.wikipedia.org/wiki/Chebyshev_polynomials), ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b74349-7ee7-4376-8541-29a3e7d45587",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_POLYNOMIAL_TERMS = 15\n",
    "\n",
    "# NumPy has a function for polynomial fits (which is analytic because\n",
    "# it can be rewritten as a linear fit in the polynomial coefficients)\n",
    "coefficients = np.polyfit(x, y, NUMBER_OF_POLYNOMIAL_TERMS - 1)[::-1]\n",
    "\n",
    "model_x = np.linspace(0, 1, 1000)\n",
    "model_y = sum(c * model_x**i for i, c in enumerate(coefficients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f08fdf-cbf4-4240-b0fc-9c1a68eafd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(x, y, marker=\".\")\n",
    "ax.plot(np.linspace(0, 1, 1000), truth(np.linspace(0, 1, 1000)), color=\"magenta\", linewidth=3)\n",
    "ax.plot(model_x, model_y, color=\"orange\", linewidth=3)\n",
    "\n",
    "ax.legend([\"measurements\", \"truth\", f\"{len(coefficients)} Taylor components\"])\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e24d9d9-791b-4ebd-9290-4ecf7bf92b3a",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8afe39-3084-4237-9d53-ff703e6f740f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_COS_TERMS = 7\n",
    "NUMBER_OF_SIN_TERMS = 7\n",
    "\n",
    "# NumPy's FFT doesn't apply because the data aren't uniformly spaced, but we can compute a Fourier series with integrals\n",
    "# Like the linear fit and the Taylor series, this is an analytic fit.\n",
    "sort_index = np.argsort(x)\n",
    "x_sorted = x[sort_index]\n",
    "y_sorted = y[sort_index]\n",
    "\n",
    "constant_term = np.trapz(y_sorted, x_sorted)\n",
    "cos_terms = [2*np.trapz(y_sorted * np.cos(2*np.pi * (i + 1) * x_sorted), x_sorted) for i in range(NUMBER_OF_COS_TERMS)]\n",
    "sin_terms = [2*np.trapz(y_sorted * np.sin(2*np.pi * (i + 1) * x_sorted), x_sorted) for i in range(NUMBER_OF_SIN_TERMS)]\n",
    "\n",
    "model_x = np.linspace(0, 1, 1000)\n",
    "model_y = (\n",
    "    constant_term +\n",
    "    sum(coefficient * np.cos(2*np.pi * (i + 1) * model_x) for i, coefficient in enumerate(cos_terms)) +\n",
    "    sum(coefficient * np.sin(2*np.pi * (i + 1) * model_x) for i, coefficient in enumerate(sin_terms))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e60631-8172-4026-8162-74ae2e98d1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(x, y, marker=\".\")\n",
    "ax.plot(np.linspace(0, 1, 1000), truth(np.linspace(0, 1, 1000)), color=\"magenta\", linewidth=3)\n",
    "ax.plot(model_x, model_y, color=\"orange\", linewidth=3)\n",
    "\n",
    "ax.legend([\"measurements\", \"truth\", f\"{1 + len(cos_terms) + len(sin_terms)} Fourier components\"])\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaebf5a5-fb12-4621-8256-a433449bb416",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfeaf2aa-71dc-4ede-8d9d-1e5d495360a8",
   "metadata": {},
   "source": [
    "Since the true function is neither polynomial nor sinusoidal, many terms are needed for convergence.\n",
    "\n",
    "When only a few terms are used, the fit has irrelevant artifacts (wiggles)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b294a1-e8c4-4962-8752-10c244db3c10",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bade19-8993-444e-9e8b-174a40c1a89e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Attempt 4: adaptive basis functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5804ee9a-5434-4386-a044-13a23df0ac53",
   "metadata": {},
   "source": [
    "The classical methods (Taylor, Fourier, etc.) have one thing in common: they all use a fixed function $\\psi_i$ for each $i$:\n",
    "\n",
    "$$ f(x) = \\sum_i^N c_i \\psi_i(x) $$\n",
    "\n",
    "and are only allowed to optimize the coefficients $c_i$ in front of each function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf830e4-768a-44d9-a735-8e0528399800",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229aef27-cf3e-4572-a5e2-2fe5a6917124",
   "metadata": {},
   "source": [
    "Suppose, instead, we had a set of functions that could also _change shape_:\n",
    "\n",
    "$$ f(x) = \\sum_i^N c_i \\psi(x; \\alpha_i, \\beta_i) $$\n",
    "\n",
    "For instance, the functions are sigmoids whose center $\\alpha$ and width $\\beta$ are adjustable:\n",
    "\n",
    "$$ \\psi(x; \\alpha, \\beta) = \\frac{1}{1 + \\exp\\big((x - \\alpha)/\\beta\\big)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a9bc1a-2786-4f50-ab61-e9f8ad439354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_component(x, center, width):\n",
    "    # ignore NumPy errors when Minuit explores extreme values\n",
    "    with np.errstate(over=\"ignore\", divide=\"ignore\"):\n",
    "        return 1 / (1 + np.exp((x - center) / width))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49dce2d-5500-470a-b60f-8b4f3bb6749b",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb58955d-cce9-4d7e-93ba-991dfbd095b1",
   "metadata": {},
   "source": [
    "A few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20e2765-33f8-47a8-a091-8c7a4e21e257",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "model_x = np.linspace(0, 1, 1000)\n",
    "\n",
    "ax.plot(model_x, sigmoid_component(model_x, 0.5, 0.2))\n",
    "ax.plot(model_x, sigmoid_component(model_x, 0.5, 0.1))\n",
    "ax.plot(model_x, sigmoid_component(model_x, 0.75, -0.01))\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cc9af0-dddc-47db-926c-752251e17764",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80ea41e-b3a4-4fca-9dc2-80b88ef0fcee",
   "metadata": {},
   "source": [
    "Now use the adaptive sigmoids in the fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c07a1f4-3912-41ff-a43c-994d8d583fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_SIGMOIDS = 5\n",
    "\n",
    "def sigmoid_sum(x, parameters):\n",
    "    out = np.zeros_like(x)\n",
    "    for coefficient, center, width in parameters.reshape(-1, 3):\n",
    "        out += coefficient * sigmoid_component(x, center, width)\n",
    "    return out\n",
    "\n",
    "# using Minuit again\n",
    "least_squares = LeastSquares(x, y, 0.03, sigmoid_sum)\n",
    "\n",
    "# do best of 15 optimizations because this space has a lot more local minima\n",
    "best_minimizer = None\n",
    "for iteration in range(15):\n",
    "\n",
    "    initial_parameters = np.zeros(5 * 3)\n",
    "    initial_parameters[0::3] = np.random.normal(0, 1, NUMBER_OF_SIGMOIDS)    # coefficient terms\n",
    "    initial_parameters[1::3] = np.random.uniform(0, 1, NUMBER_OF_SIGMOIDS)   # center parameters (alpha)\n",
    "    initial_parameters[2::3] = np.random.normal(0, 0.1, NUMBER_OF_SIGMOIDS)  # width parameters (beta)\n",
    "\n",
    "    minimizer = iminuit.Minuit(least_squares, initial_parameters)\n",
    "    minimizer.migrad()\n",
    "\n",
    "    if best_minimizer is None or minimizer.fval < best_minimizer.fval:\n",
    "        best_minimizer = minimizer\n",
    "\n",
    "model_x = np.linspace(0, 1, 1000)\n",
    "model_y = sigmoid_sum(model_x, np.array(best_minimizer.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bec7010-839c-4384-ad04-751b41638740",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(x, y, marker=\".\")\n",
    "ax.plot(np.linspace(0, 1, 1000), truth(np.linspace(0, 1, 1000)), color=\"magenta\", linewidth=3)\n",
    "ax.plot(model_x, model_y, color=\"orange\", linewidth=3)\n",
    "\n",
    "ax.legend([\"measurements\", \"truth\", f\"{len(minimizer.parameters)} sigmoid parameters\"])\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebdab6f-ac37-4aac-8d31-1e8340cf6141",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b63445-2a98-4fcc-bd60-093c19afdad8",
   "metadata": {},
   "source": [
    "The fitter doesn't need very many sigmoids because it can position each one and stretch it arbitrarily.\n",
    "\n",
    "It can even stack them to build piecewise shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8e2113-2c1a-446e-b33b-a925011f6aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "model_x = np.linspace(0, 1, 1000)\n",
    "\n",
    "wide_plateau_left = sigmoid_component(model_x, 0.2, 0.005)\n",
    "wide_plateau_right = sigmoid_component(model_x, 0.9, -0.005)\n",
    "\n",
    "narrow_peak_left = sigmoid_component(model_x, 0.4, 0.005)\n",
    "narrow_peak_right = sigmoid_component(model_x, 0.6, -0.005)\n",
    "\n",
    "ax.plot(model_x, -wide_plateau_left - wide_plateau_right - narrow_peak_left - narrow_peak_right)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f30c426-ab88-4e3b-81b6-bf1ca4c56225",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ae6d48-a101-4fa7-93b4-d8c89a1fa233",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### The important point: adaptive basis functions _are_ a neural network layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56698567-eff1-409a-bbff-851409f438f3",
   "metadata": {},
   "source": [
    "Instead of a parameterized sigmoid,\n",
    "\n",
    "$$ \\psi(x; \\alpha, \\beta) = \\frac{1}{1 + \\exp\\big((x - \\alpha)/\\beta\\big)} $$\n",
    "\n",
    "consider applying a linear transformation to the input of a sigmoid:\n",
    "\n",
    "$$\n",
    "x^{\\mbox{\\scriptsize layer 2}} = \\frac{x^{\\mbox{\\scriptsize layer 1}} - \\alpha}{\\beta}\n",
    "\\mbox{\\hspace{1 cm}and\\hspace{1 cm}}\n",
    "f(x^{\\mbox{\\scriptsize layer 2}}) = \\frac{1}{1 + \\exp\\big( x^{\\mbox{\\scriptsize layer 2}} \\big)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f89c85-002d-4baa-b991-99726a5463ab",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be8679a-8631-48ef-824e-55f84f241765",
   "metadata": {},
   "source": [
    "5 independently scaled sigmoids are a single hidden layer with 5 nodes:\n",
    "\n",
    "$$\n",
    "y =\n",
    "c_i^{\\mbox{\\scriptsize L2--L3}} \\cdot\n",
    "f\\left(\\frac{x - \\alpha_i^{\\mbox{\\scriptsize L1-L2}}}{\\beta_i^{\\mbox{\\scriptsize L1-L2}}}\\right)\n",
    "$$\n",
    "\n",
    "The 5 $\\alpha$ and 5 $\\beta$ parameters are the linear transformation from the input layer 1 to the hidden layer 2, the sigmoid $f$ is the activation function applied at this layer, and the coefficients in front of each sigmoid $c$ are the linear transformation from layer 2 to the output layer 3.\n",
    "\n",
    "<img src=\"../img/artificial-neural-network-layers-3.svg\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97272c3-c143-4900-ae05-07d48803b87f",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1b5c79-bc71-4cb4-929c-a97de19b0bf9",
   "metadata": {},
   "source": [
    "To further demonstrate this, let's use a neural network implementation from Scikit-Learn to fit the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abeaea22-b172-42bc-9bff-d36b3a503d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.neural_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d601b6a-14e3-4417-82f7-dbe753ec8d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do best of 15 optimizations because this space has a lot of local minima\n",
    "best_neural_network = None\n",
    "for iteration in range(15):\n",
    "\n",
    "    # Scikit-Learn's MLPRegressor uses ordinary least squares as a loss function\n",
    "    # the \"logistic\" activation function is our sigmoid\n",
    "    neural_network = sklearn.neural_network.MLPRegressor(\n",
    "        activation=\"logistic\", hidden_layer_sizes=(5,),\n",
    "        solver=\"lbfgs\", max_iter=10000, alpha=0,\n",
    "    )\n",
    "    \n",
    "    neural_network.fit(x[:, np.newaxis], y)\n",
    "\n",
    "    if best_neural_network is None or neural_network.loss_ < best_neural_network.loss_:\n",
    "        best_neural_network = neural_network\n",
    "\n",
    "model_x = np.linspace(0, 1, 1000)\n",
    "model_y = best_neural_network.predict(model_x[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebae1b7-523d-4fd1-abb2-c51396922cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(x, y, marker=\".\")\n",
    "ax.plot(np.linspace(0, 1, 1000), truth(np.linspace(0, 1, 1000)), color=\"magenta\", linewidth=3)\n",
    "ax.plot(model_x, model_y, color=\"orange\", linewidth=3)\n",
    "\n",
    "num_params = sum(x.size for x in neural_network.coefs_) + sum(x.size for x in neural_network.intercepts_)\n",
    "ax.legend([\"measurements\", \"truth\", f\"{num_params} parameters\"])\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad1c14c-89b3-47ad-8a9d-dcb7603cd237",
   "metadata": {},
   "source": [
    "(A real neural network has one more bias term per output variable than our analogy, so 16 parameters, not 15.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf34a99-ff90-40ad-bbdb-3067b70238c2",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5069b2fc-b2c0-4973-b9bd-5a740aac09e6",
   "metadata": {},
   "source": [
    "Thus, a neural network with a hidden layer is a function approximator like Taylor and Fourier series, but with a special property: _the basis functions are adaptive_.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "One consequence of this is that the basis functions are not orthogonal, like Taylor and Fourier series.\n",
    "\n",
    "* Since Taylor and Fourier basis functions are orthogonal, each coefficient can be determined independently.\n",
    "* Since a neural network's adaptive basis functions are not, they _must_ be determined by a combined fit.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "In fact, many parameters can be swapped: if $\\{ \\alpha_i \\mbox{, } \\beta_i \\mbox{, } c_i \\} \\longleftrightarrow \\{ \\alpha_j \\mbox{, } \\beta_j \\mbox{, } c_j \\}$ for sigmoids $i$ and $j$, the function output is unchanged.\n",
    "\n",
    "* Each minimum in the optimizer's objective function has $n!$ identical minima, for each hidden layer of size $n$.\n",
    "* Two neural networks that return the same output for all possible input could have very different internal parameters.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "[It's a bumpy objective function!](https://www.cs.umd.edu/~tomg/projects/landscapes/)\n",
    "\n",
    "<img src=\"../img/loss-visualization-noshort.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fd09f7-07d0-4449-909c-41729dffd5e6",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194758e7-3164-416a-892b-afce7b69644b",
   "metadata": {},
   "source": [
    "**This is what I mean by \"farming\": ML doesn't eliminate all difficulties, it replaces one set of problems with another.**\n",
    "\n",
    "<table>\n",
    "    <tr style=\"background: white\";>\n",
    "        <td><img src=\"make-talk/img/craftsmanship.jpg\" width=\"300\"></td>\n",
    "        <td><img src=\"make-talk/img/farming.jpg\" width=\"300\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Optimizing a neural network requires far less detailed knowledge of the function than the ansatz fit.\n",
    "\n",
    "But it requires more attention to the network architecture and the minimizer.\n",
    "\n",
    "* This is just another tool; sometimes one tool is better for a particular problem, sometimes another.\n",
    "* However, these \"farming\" problems are more generic: what computer scientists learn about minimization algorithms in general may be directly applicable to your task.\n",
    "\n",
    "Today, the best practice for minimization is to [use Adam or AdamW](https://www.fast.ai/posts/2018-07-02-adam-weight-decay.html) and try different choices of learning rate and regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3e02d1-c94f-42ae-8cc1-164f64e5a0ed",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d1191f-a652-439a-a2e5-6dbfaa481f91",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 20 minute exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815aae86-c78c-4335-b271-2b1b5bae11f6",
   "metadata": {},
   "source": [
    "Before we start coding, let's get familiar with training neural networks in a graphical interface:\n",
    "\n",
    "<a href=\"https://playground.tensorflow.org/#activation=sigmoid&batchSize=10&dataset=gauss&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=&seed=0.91883&showTestData=false&discretize=false&percTrainData=50&x=false&y=false&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=regression&initZero=false&hideText=false\"><img src=\"../img/tensorflow-playground.png\" width=\"100%\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906617de-7dc3-4288-8f78-15be8f1c4a92",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ebfcbb-e944-48ee-a472-3d9920aa39d6",
   "metadata": {},
   "source": [
    "Your job is to fit each of the training datasets with as few neurons as possible (counting each input feature as a neuron, just as hidden layers are neurons).\n",
    "\n",
    "First, add your name to the [scoreboard Google Doc](https://docs.google.com/spreadsheets/d/1nRtNJoxW1i-jCr04ZHUlfv0DU4tMGbyvCZcpXakYedE/edit?usp=sharing).\n",
    "\n",
    "Then, try to reach or beat my best score and report it there.\n",
    "\n",
    "At the end, we'll share solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee1dfa3-1834-4382-91d8-4c76a6fe0c99",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Outline of the rest of this mini-course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d066762-04d2-4695-b06c-5b7a50bedd09",
   "metadata": {},
   "source": [
    "* **Introduction (1.5 hours)**\n",
    "  * Craftsmanship versus farming\n",
    "  * History of HEP + ML\n",
    "  * Universal approximation theorem(s)\n",
    "  * 20 minute playground.tensorflow.org exercise\n",
    "* **Issues in practice (2 hours)**\n",
    "  * Which library?\n",
    "  * Regression versus classification, loss functions\n",
    "  * Optimizers: learning rate, batches, and epochs\n",
    "  * Feature selection and the \"kernel trick\"\n",
    "  * Under & overfitting\n",
    "  * Parameters versus hyperparameters\n",
    "  * Partitioning data into train-test-validate\n",
    "  * Goodness of fit metrics\n",
    "  * Regularization: L1, L2, dropout\n",
    "* **Survey of architectures (1.5 hours)**\n",
    "  * What are the building blocks?\n",
    "  * Multilayer perceptron\n",
    "  * Autoencoder and variational autoencoder\n",
    "  * Convolutional Neural Networks (CNNs)\n",
    "  * DeepSet and Graph Neural Networks (GNNs)\n",
    "  * Generative Adversarial Networks (GANs)\n",
    "  * Recurrent Neural Network (RNNs)\n",
    "  * Transformers & ChatGPT\n",
    "* **Main exercise (2 hours)**\n",
    "  * Gather into small teams\n",
    "  * Classify jets by substructure"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
