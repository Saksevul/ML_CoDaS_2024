{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4cda695-e7bb-402d-847c-af5d9092c84b",
   "metadata": {},
   "source": [
    "# Machine Learning at CoDaS-HEP 2024, Lesson 4: Survey of Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c63bb9d-ce3a-474f-acf7-de7c2bd95925",
   "metadata": {},
   "source": [
    "In lesson 1, I introduced neural networks and the universal function approximation theorem. A single hidden layer implements _adaptive_ basis functions, more flexible than classic Taylor and Fourier series.\n",
    "\n",
    "In lesson 2, we talked about issues involed in any fitting procedure, whether multilayered or not (i.e. a pure linear fit).\n",
    "\n",
    "Lesson 3 was an open-ended project to build your own neural network.\n",
    "\n",
    "In lesson 4, we will consider a variety of neural network \"architectures\": ways of building networks to improve learning for different types of problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f3c63e-1d3c-40db-9af2-59cab7034f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import awkward as ak\n",
    "\n",
    "import sklearn.datasets\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89a9945-0844-4119-b452-6bedbab69fa0",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd9c33a-086e-4711-bfc8-ba8d2ba3dbae",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Why should learning be \"deep\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7ce9d2-f8c5-474d-8cae-a0a7e007bb41",
   "metadata": {},
   "source": [
    "**Deep learning:** a neural network with 3 or more layers (which is common nowadays).\n",
    "\n",
    "<img src=\"../img/rise-of-deep-learning.svg\" width=\"800\">\n",
    "\n",
    "* 2006‒2007: problems that _prevented_ the training of deep learning were solved.\n",
    "* 2012: AlexNet, a GPU-enabled 8 layer network (with ReLU), won the ImageNet competition.\n",
    "* 2015: ResNet, a GPU-enabled 152+ layer network (with skip-connections), won the ImageNet competition.\n",
    "\n",
    "By 2015, it was clear that networks with many layers have more potential than one big hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63ab18f-6916-40cf-8c04-c2b7598f6161",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ce597b-9a7e-44aa-b98a-e24e76e4d724",
   "metadata": {},
   "source": [
    "Why does it work?\n",
    "\n",
    "One big hidden layer can approximate any shape, by optimizing adaptive basis functions, but according to [conventional wisdom](https://stats.stackexchange.com/a/223637/36505),\n",
    "\n",
    "> Shallow networks are very good at memorization, but not so good at generalization.\n",
    "\n",
    "That is, they have a tendency to overfit.\n",
    "\n",
    "Why are multiple layers better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24745c8f-8b38-469a-87b1-6a2ba9e5c56a",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faef60c-10ce-4dd9-9c09-1dcbcee9a6d1",
   "metadata": {},
   "source": [
    "Reminder of adaptive basis function:\n",
    "\n",
    "$$ \\psi(x; a, b) = \\left\\{\\begin{array}{c l}\n",
    "a + b x & \\mbox{if } x > -a/b \\\\\n",
    "0 & \\mbox{otherwise} \\\\\n",
    "\\end{array}\\right. $$\n",
    "\n",
    "Function approximation with one hidden layer:\n",
    "\n",
    "$$ f_j(x) = \\sum_i^{N_1} \\psi(x; a_{ij}, b_{ij}) c_{ij} $$\n",
    "\n",
    "Function approximation with two hidden layers:\n",
    "\n",
    "$$ f_k(x) = \\sum_j^{N_2} \\psi\\left(x; \\left[\n",
    "\\sum_i^{N_1} \\psi(x; a_{i1}, b_{i1}) c_{i1}\n",
    "\\right], \\left[\n",
    "\\sum_i^{N_1} \\psi(x; a_{i2}, b_{i2}) c_{i2}\n",
    "\\right]\\right) \\left[\n",
    "\\sum_i^{N_1} \\psi(x; a_{i3}, b_{i3}) c_{i3}\n",
    "\\right] $$\n",
    "\n",
    "And so on: adaptively adaptive basis functions, then adaptively adaptively adaptive basis functions..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d135cb58-4ae9-4197-898f-88746d66a86f",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3234da30-09e3-4683-8c67-4235b112930e",
   "metadata": {},
   "source": [
    "* Adding one more neuron in a single layer adds a wiggle to the fit function.\n",
    "* Adding one more layer effectively folds the space under the next set of wiggly functions. Instead of fitting individual wiggles, they find symmetries in the data that (probably) correspond to an underlying relationship, rather than noise.\n",
    "\n",
    "Consider this horseshoe-shaped decision boundary: with two well-chosen folds along the symmetries, it reduces to a simpler curve to fit. Instead of 4 ad-hoc wiggles, it's 2 folds and 1 wiggle.\n",
    "\n",
    "<img src=\"../img/deep-learning-by-space-folding.svg\" width=\"800\">\n",
    "\n",
    "Montúfar, Pascanu, Cho, & Bengio, [_On the Number of Linear Regions of Deep Neural Networks_](https://arxiv.org/abs/1402.1869) (2014)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70914eca-bf0f-4858-884f-d8d9d35586e8",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4339d9bb-5af1-47c2-8274-563233e22acc",
   "metadata": {},
   "source": [
    "Roy Keyes's fantastic demo ([with code](https://gist.github.com/jpivarski/f99371614ecaa48ace90a6025d430247)):\n",
    "\n",
    "<img src=\"../img/network-layer-space-folding.png\" width=\"800\">\n",
    "\n",
    "A uniform grid on the feature space (left; grid not shown) projected through the first layer's transformation shows what the underlying space looks like (right; grid is gray) before the second layer makes a linear decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43466ae2-3dc7-4f33-ae63-f2aa5970e7ef",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c208726-9aec-48dc-8dba-5b7fc9ae7e3a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "This is our first architecture, just a feed-forward \"Multi Layer Perceptron\" (MLP):\n",
    "\n",
    "<img src=\"../img/artificial-neural-network-layers-2.svg\" width=\"700\">\n",
    "\n",
    "Neurons in a layer add wiggles to the fitted function; layers add reflections and symmetries that are (probably) real structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd428bb-ddac-4ad7-9dfd-2390d6536785",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abed1d9-e14d-4a65-9e98-a9c19cc7b8c2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f6add5-4f44-4448-bc5e-de2b0f3d2838",
   "metadata": {},
   "source": [
    "Here is our second architecture, an **autoencoder**:\n",
    "\n",
    "<img src=\"../img/artificial-neural-network-layers-autoencoder.svg\" width=\"700\">\n",
    "\n",
    "The network structure is qualitatively like the first; the only difference is that the number of neurons shrinks to a \"pinch point\" and then returns to the original size.\n",
    "\n",
    "The training is different: instead of trying to fit to known targets, we train the model to produce output that matches the input. When fully trained, it approximates the identity function.\n",
    "\n",
    "This is **unsupervised learning**. Unlike **supervised learning**, in which we want the model to produce an expected answer, we let this model examine the data and come up with something on its own.\n",
    "\n",
    "Before talking about it in detail, let's run an autoencoder on the same jet data that you classified with supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f8e71a-61e2-458c-a58c-e8d1fc8800f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hls4ml_lhc_jets_hlf = sklearn.datasets.fetch_openml(\"hls4ml_lhc_jets_hlf\")\n",
    "\n",
    "features_unnormalized = torch.tensor(hls4ml_lhc_jets_hlf[\"data\"].values).float()\n",
    "\n",
    "features = (features_unnormalized - features_unnormalized.mean(axis=0)) / features_unnormalized.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d23a59-2294-45bf-ab3f-8abb36a0de19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.shrinking = nn.Sequential(\n",
    "            nn.Linear(16, 12),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(12, 8),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(8, 4),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(4, 2),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        self.growing = nn.Sequential(\n",
    "            nn.Linear(2, 4),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(4, 8),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(8, 12),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(12, 16),\n",
    "        )\n",
    "\n",
    "    def forward(self, features):\n",
    "        return self.growing(self.shrinking(features))\n",
    "\n",
    "model = Autoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5111ffd9-9393-46fc-b336-efe136f79080",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 1000\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.03)\n",
    "\n",
    "loss_vs_epoch = []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    total_loss = 0\n",
    "\n",
    "    for start_batch in range(0, len(features), BATCH_SIZE):\n",
    "        stop_batch = start_batch + BATCH_SIZE\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        predictions = model(features[start_batch:stop_batch])\n",
    "        loss = loss_function(predictions, features[start_batch:stop_batch])\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    loss_vs_epoch.append(total_loss)\n",
    "    print(f\"{epoch = } {total_loss = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e819dd-0f08-4fde-ae19-b1932507cedb",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f584d797-b928-42c5-93a0-9f43515e0322",
   "metadata": {},
   "source": [
    "To reproduce the 16-dimensional input data using only 2 dimensions in the middle, the model has to encode it with as little redundancy as possible.\n",
    "\n",
    "We are asking the model to perform lossy compression—to _approximate_ the 16-dimensional data in 2 dimensions.\n",
    "\n",
    "The data in 2 dimensional space looks very different from how it looks in the original 16 dimensions, but the biggest distinctions in one space are big distinctions in the other. This is an **embedding space** for the data.\n",
    "\n",
    "Aside: in text processing networks, the physical space consists of exact words and the embedding space consists of _meanings_, which might not be one-to-one with words. (In this embedding space, relationships like\n",
    "\n",
    "$$ \\mbox{king} - \\mbox{man} + \\mbox{woman} = \\mbox{queen} $$\n",
    "\n",
    "hold.)\n",
    "\n",
    "<img src=\"../img/neural-network-for-language.svg\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c63e0ff-1d9e-469a-b503-435ffe8f64e6",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80c7afe-91b8-4cc2-8fe8-edc58b0e5635",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(range(len(loss_vs_epoch)), loss_vs_epoch)\n",
    "ax.set_xlim(-1, len(loss_vs_epoch))\n",
    "ax.set_ylim(0, 1.1*max(loss_vs_epoch))\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(\"loss\")\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4223f039-a5e6-439d-b5b8-497ceb663005",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d61d2d-b7dd-4441-859b-b31ca3faeeea",
   "metadata": {},
   "source": [
    "What does the data look like in the 2 dimensional space at the \"pinch point\" of the network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28eb7db8-8902-4211-95b0-cf5114ff660b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded = model.shrinking(features).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6c2184-c451-443d-8fd8-002d829e8008",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "\n",
    "p = ax.hist2d(embedded[:, 0], embedded[:, 1], bins=(100, 100), range=((0, 1), (0, 1)), norm=mpl.colors.LogNorm())\n",
    "fig.colorbar(p[-1], ax=ax, label=\"number of samples\")\n",
    "ax.axis([0, 1, 0, 1])\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d8a1af-aded-46cb-92c8-f74d3d0e5b1e",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f370749-4d9b-40c3-b384-9ca13b607ee2",
   "metadata": {},
   "source": [
    "The model found some clumps; some clusters of different-looking jets.\n",
    "\n",
    "Do these correspond to the `'g'`, `'q'`, `'t'`, `'w'`, `'z'` categories, the physically different hadronization mechanisms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23205d3-96fa-4116-90ac-6d12395f8afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torch.tensor(hls4ml_lhc_jets_hlf[\"target\"].cat.codes.values).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041200b9-4a0b-471c-8ec6-70e620a02686",
   "metadata": {},
   "outputs": [],
   "source": [
    "hls4ml_lhc_jets_hlf[\"target\"].cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f8fef0-6d48-45f8-9eaf-a5f17ecf7637",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_g = model.shrinking(features[targets == 0]).detach().numpy()\n",
    "embedded_q = model.shrinking(features[targets == 1]).detach().numpy()\n",
    "embedded_t = model.shrinking(features[targets == 2]).detach().numpy()\n",
    "embedded_w = model.shrinking(features[targets == 3]).detach().numpy()\n",
    "embedded_z = model.shrinking(features[targets == 4]).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327fa4da-f26f-4a1a-a06c-0cfaff780c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5, 1, figsize=(6.5, 30))\n",
    "\n",
    "ps = []\n",
    "for (ax, (name, embedded)) in zip(axs, [\n",
    "    [\"g\", embedded_g], [\"q\", embedded_q], [\"t\", embedded_t], [\"w\", embedded_w], [\"z\", embedded_z]\n",
    "]):\n",
    "    ps.append(ax.hist2d(embedded[:, 0], embedded[:, 1], bins=(100, 100), range=((0, 1), (0, 1)), norm=mpl.colors.LogNorm()))\n",
    "    fig.colorbar(ps[-1][-1], ax=ax, label=\"number of samples\")\n",
    "    ax.scatter([embedded[:, 0].mean()], [embedded[:, 1].mean()], marker=\"*\", s=600, color=\"white\")\n",
    "    ax.scatter([embedded[:, 0].mean()], [embedded[:, 1].mean()], marker=\"*\", s=300, color=\"red\")\n",
    "    ax.set_title(f\"distribution of '{name}' jets\")\n",
    "    ax.axis([0, 1, 0, 1])\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d1a487-e4fa-4157-bf75-852acc7dbc58",
   "metadata": {},
   "source": [
    "Not quite. `'g'`, `'q'`, `'t'` populate different clusters from each other, although the model split them up with more granularity.\n",
    "\n",
    "The `'w'`, `'z'` are different from the quark-gluon jets, but not different from each other.\n",
    "\n",
    "It would be interesting to map these clusters back to the original 16-dimensional jets, to understand what these phenominological clusters mean, but not now.\n",
    "\n",
    "Moving on!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b2b72e-f83d-4a65-afab-6649b20ba215",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3968d1bd-1cff-4f22-9495-86b6d1957b0f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Variational autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a226692-121a-436a-afd4-fd14972ef88c",
   "metadata": {},
   "source": [
    "Since we're interested in clusters in the autoencoder's \"pinch point,\" why not encode them as distributions?\n",
    "\n",
    "* An ordinary autoencoder maps input data to _points_ in a small-dimensional space, such as $(x_1, x_2, \\ldots x_n)$.\n",
    "* A variational autoencoder maps input data to _parameters of distributions_, such as $(\\mu_1, \\sigma_1, \\mu_2, \\sigma_2, \\ldots, \\mu_n, \\sigma_n)$.\n",
    "\n",
    "Values in the next layer are randomly generated from these distributions.\n",
    "\n",
    "Thus, there are now three types of vector-transformation in the neural networks we have considered:\n",
    "\n",
    "1. linear transformations\n",
    "2. non-linear activation functions\n",
    "3. random generation from distribution parameters.\n",
    "\n",
    "Moving on!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509c7fb6-546e-4dad-be4b-249ce5e0e0fe",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7286597-56b7-4044-adeb-8a65319ec043",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Convolutional neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c96a62d-0e66-4f3f-9a79-219068d5d0bc",
   "metadata": {},
   "source": [
    "The jet substructure dataset that we have been fitting consists of 16 hand-crafted features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec35f5e-6246-4cd8-b740-22f286de5f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(hls4ml_lhc_jets_hlf[\"data\"].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69660ac1-0127-4493-a59f-5bab9a027fb6",
   "metadata": {},
   "source": [
    "But what if we don't know what are the best features to use?\n",
    "\n",
    "What if these 16 aren't the best features?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b785aa8-3237-4275-b217-87e94eec555e",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d01ba6e-097b-42b5-87f5-07203eceecb8",
   "metadata": {},
   "source": [
    "Suppose, instead, we start with the raw data (ECAL and HCAL clusters).\n",
    "\n",
    "The jet substructure is presented in its lowest-level form: images.\n",
    "\n",
    "This file contains individual jet images, labeled by `'g'`, `'q'`, `'t'`, `'w'`, `'z'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1370e7dd-0e1a-4c3a-a996-7080e716b4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"../data/jet-images.h5\") as file:\n",
    "    jet_images = file[\"images\"][:]\n",
    "    jet_labels = file[\"labels\"][:]\n",
    "\n",
    "jet_label_order = [\"g\", \"q\", \"t\", \"w\", \"z\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd592eb7-7c3a-4beb-b94c-5cff5ddf0682",
   "metadata": {},
   "source": [
    "There are $80\\,000$ images with 20×20 pixels each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b28748-a05c-4298-b963-a6e6f08983b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "jet_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be872ca6-b6b5-4b1f-949d-96f567b88a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4, 4, figsize=(12, 12))\n",
    "\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    ax.imshow(jet_images[i])\n",
    "    ax.text(10, 1.5, f\"'{jet_label_order[jet_labels[i]]}' jet image\", color=\"white\")\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090f2171-5af0-4c9e-a066-6ff286289f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5, 1, figsize=(6, 30))\n",
    "\n",
    "for i, ax in enumerate(axs):\n",
    "    ax.imshow(np.sum(jet_images[jet_labels == i], axis=0))\n",
    "    ax.set_title(f\"sum of '{jet_label_order[i]}' jet images\")\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccaf04f-5730-430d-b651-5530124a528c",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db057a22-73f7-4d91-9e17-5829e8f7d904",
   "metadata": {},
   "source": [
    "Instead of a 16-dimensional input space, this dataset has a 400-dimensional input space (20 pixels times 20 pixels).\n",
    "\n",
    "Just one fully connected layer would be a matrix with $400^2 = 160\\,000$ parameters to fit.\n",
    "\n",
    "We only have $80\\,000$ images, so it would be highly overfitted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86a85b3-6d60-4543-9022-09da72e61da2",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5f9491-0e9b-4b3d-8722-42c04bf502f0",
   "metadata": {},
   "source": [
    "Taking inspiration from biology (again):\n",
    "\n",
    "<img src=\"../img/eye-neurons.jpg\" width=\"400\">\n",
    "\n",
    "Photoreceptors for distant spatial points are not directly connected. Only nearby points are connected in the first layer.\n",
    "\n",
    "<img src=\"../img/convolutional-planes.png\" width=\"400\">\n",
    "\n",
    "A linear transformation of only nearby points is known as a [convolution](https://en.wikipedia.org/wiki/Convolution), so this is called a **Convolutional Neural Network** (CNN)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393b4c07-bf38-403e-aa16-54f77826dd40",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a0e770-81dc-4b46-a2f3-420c277bf796",
   "metadata": {},
   "source": [
    "Before talking about it in detail, let's run a convolutional network on the jet images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049b5345-8875-4031-9993-4d1e70c8d29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "jet_images_tensor = torch.tensor(jet_images)[:, np.newaxis, :, :]\n",
    "jet_labels_tensor = torch.tensor(jet_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59bf232-b97b-44d4-a55b-d37fd7cbf359",
   "metadata": {},
   "source": [
    "PyTorch wants this shape: (number of images, number of channels, height in pixels, width in pixels), so we make the 1 channel explicit with `np.newaxis`.\n",
    "\n",
    "(An RGB image would have 3 channels, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54170cb5-f453-490d-b5b5-d1a48778e68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "jet_images_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ff035a-5ffe-4c92-992e-f7187b4c00d7",
   "metadata": {},
   "source": [
    "A PyTorch [nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) has enough tunable parameters to describe a fixed-size convolution matrix (3×3 below) from a number of input channels (1 below) to a number of output channels (1 below).\n",
    "\n",
    "The number of parameters _does not_ scale with the size of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560aedb8-d798-4ede-aee9-15d0a1e44eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(nn.Conv2d(1, 1, 3).parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad69c9c-4409-434d-91fc-de2958c58d98",
   "metadata": {},
   "source": [
    "A PyTorch [nn.MaxPool2d](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html) scales down an image by a fixed factor, by taking the maximum value in every $n \\times n$ block.\n",
    "\n",
    "It has _no_ tunable parameters.\n",
    "\n",
    "Although not strictly necessary, it's a generally useful practice to pool convolutions, to reduce the total number of parameters and sensitivity to noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ec1e6d-b5a7-486d-8faf-384ea9505c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(nn.MaxPool2d(2).parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdab69da-a652-4cb7-aa5a-12a55c80b503",
   "metadata": {},
   "source": [
    "The general strategy is to reduce the size of the image with each convolution (and max-pooling) while increasing the number of channels, so that the spatial grid gradually becomes an abstract vector.\n",
    "\n",
    "Then do a normal fully-connected network to classify the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6aa1f82-a40f-47e9-b98c-aefab30e57bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.convolutional1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 5, 5),     # 1 input channel → 5 output channels, 5×5 convolution...\n",
    "            nn.ReLU(),              #     input image: 20×20, convoluted image: 16×16 (because of edges)\n",
    "            nn.MaxPool2d(2),        # scales down by taking the max in 2×2 squares, output is 8×8\n",
    "        )\n",
    "        self.convolutional2 = nn.Sequential(\n",
    "            nn.Conv2d(5, 10, 5),    # 5 input channels → 10 output channels, 5×5 convolution...\n",
    "            nn.ReLU(),              #     input image: 8×8, convoluted image: 4×4 (because of edges)\n",
    "            nn.MaxPool2d(2),        # scales down by taking the max in 2×2 squares, output is 2×2\n",
    "        )\n",
    "        self.fully_connected = nn.Sequential(\n",
    "            nn.Linear(10 * 2*2, 30),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(30, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 5),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fully_connected(torch.flatten(self.convolutional2(self.convolutional1(x)), 1))\n",
    "\n",
    "model_without_softmax = ConvolutionalClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e53159b-ecd7-4a3c-8062-83d925c301a8",
   "metadata": {},
   "source": [
    "Although this has a lot of parameters ($3\\,505$), it's less than the number of images ($80\\,000$), which is much less than the number of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696add28-8942-4282-92b0-68665279b11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_model_parameters = 0\n",
    "for tensor_parameter in model_without_softmax.parameters():\n",
    "    num_model_parameters += tensor_parameter.detach().numpy().size\n",
    "\n",
    "num_model_parameters, len(jet_images_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c457e46-d128-4176-bc67-f3bb23121a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 30\n",
    "BATCH_SIZE = 1000\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(model_without_softmax.parameters(), lr=0.03)\n",
    "\n",
    "loss_vs_epoch = []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    total_loss = 0\n",
    "\n",
    "    for start_batch in range(0, len(jet_images_tensor), BATCH_SIZE):\n",
    "        stop_batch = start_batch + BATCH_SIZE\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        predictions = model_without_softmax(jet_images_tensor[start_batch:stop_batch])\n",
    "        loss = loss_function(predictions, jet_labels_tensor[start_batch:stop_batch])\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    loss_vs_epoch.append(total_loss)\n",
    "    print(f\"{epoch = } {total_loss = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31652c2-c201-45a0-9f9a-623d279f7fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(range(len(loss_vs_epoch)), loss_vs_epoch)\n",
    "ax.set_xlim(-1, len(loss_vs_epoch))\n",
    "ax.set_ylim(0, 1.1*max(loss_vs_epoch))\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(\"loss\")\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90342ff1-daf4-4bea-9417-8cb56358183b",
   "metadata": {},
   "source": [
    "Let's see the accuracy, in terms of the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f43bbfe-c42b-430f-bc8c-3566ace553f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_softmax = nn.Sequential(\n",
    "    model_without_softmax,\n",
    "    nn.Softmax(dim=1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e82137-2d45-4f60-acbe-d9860f6f1f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_tensor = model_with_softmax(jet_images_tensor)\n",
    "\n",
    "confusion_matrix = np.array(\n",
    "    [\n",
    "        [\n",
    "            (predictions_tensor[jet_labels_tensor == true_class].argmax(axis=1) == prediction_class).sum().item()\n",
    "            for prediction_class in range(5)\n",
    "        ]\n",
    "        for true_class in range(5)\n",
    "    ]\n",
    ")\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744f0fc2-8ff0-4b9c-ba43-7c31a568a281",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "image = ax.imshow(confusion_matrix, vmin=0)\n",
    "fig.colorbar(image, ax=ax, label=\"number of test samples\", shrink=0.8)\n",
    "\n",
    "ax.set_xticks(range(5), jet_label_order)\n",
    "ax.set_yticks(range(5), jet_label_order)\n",
    "\n",
    "ax.set_xlabel(\"predicted jet category\")\n",
    "ax.set_ylabel(\"true jet category\")\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c1afac-57c8-4cb7-a771-5c64984714c9",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c3ee74-16bb-45b7-9a45-0ba16b6391ba",
   "metadata": {},
   "source": [
    "Long before neural networks, (hand-coded) convolutions were used to detect edges in images.\n",
    "\n",
    "These are low-level features of the image.\n",
    "\n",
    "By repeating this process, convolutional neural networks \n",
    "\n",
    "<img src=\"../img/DWTBQ2-2018-ev-fig3.jpg\" width=\"800\">\n",
    "\n",
    "<br><br>\n",
    "\n",
    "Kunihiko Fukushima, [_Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position_](https://www.rctn.org/bruno/public/papers/Fukushima1980.pdf) (1980).\n",
    "\n",
    "<img src=\"../img/higher-order-features.png\" width=\"800\">\n",
    "\n",
    "<br><br>\n",
    "\n",
    "\"[Grandmother cell](https://en.wikipedia.org/wiki/Grandmother_cell)\" refers to an old hypothesis that, in the human brain, _one cell_ encodes a very high-level concept like one's grandmother.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "It sounds far fetched, but when Google trained an unsupervised convolutional network on a large set of YouTube videos, they were surprised by _one neuron_ that projected back onto the image space like this:\n",
    "\n",
    "<img src=\"../img/cat-neuron.png\" width=\"400\">\n",
    "\n",
    "This is what started the whole \"AI discovers cats on the internet\" thing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff4dec4-04f4-4f36-84b9-70c1476cf767",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae39611-a54e-4acd-85d2-35a916c8b044",
   "metadata": {},
   "source": [
    "There are now five types of vector-transformation in the neural networks we have considered:\n",
    "\n",
    "1. linear transformations\n",
    "2. non-linear activation functions\n",
    "3. random generation from distribution parameters\n",
    "4. small set of learned convolution parameters applied to large images\n",
    "5. reducing an image size with pooling.\n",
    "\n",
    "Moving on!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad88807-587f-4248-bb25-08fd4b36539a",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6bcd9b-46aa-4f88-b3ed-04e41eb47f0e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Ragged data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eae94e2-b233-4fe9-8913-3a832caec4bc",
   "metadata": {},
   "source": [
    "Suppose you have data like the following—how would you pass this into a neural network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b5213a-3343-4fc8-9e9a-5aa8d1230465",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_data = ak.from_parquet(\"../data/SMHiggsToZZTo4L.parquet\")\n",
    "event_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e67f62-7994-4ede-8aa6-d6714f8516d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_data.muon.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5411e6-02fe-4181-8c2b-5b79e2819962",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666f660c-cb3a-490f-868b-4b910d77015b",
   "metadata": {},
   "source": [
    "PyTorch has a [torch.nested.nested_tensor](https://pytorch.org/docs/stable/nested.html) that can represent ragged numerical data.\n",
    "\n",
    "(Notice that we have to turn this into Python lists first! PyTorch's [issue #112509](https://github.com/pytorch/pytorch/issues/112509) asks to fix this, and TensorFlow's [tf.RaggedTensor](https://www.tensorflow.org/guide/ragged_tensor) doesn't have this problem.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8faec8b-c74b-4bf9-b4e8-f4a98f0fb3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "muon_pt_tensor = torch.nested.nested_tensor(ak.to_list(event_data.muon.pt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde6ecde-f8b8-4cc8-973d-6fcfcc956b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "muon_pt_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb45645-a1f7-4cda-b376-3f555960fc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "muon_pt_tensor[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05849f0a-424d-4912-808c-dc80460df252",
   "metadata": {},
   "outputs": [],
   "source": [
    "muon_pt_tensor[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626bbfde-a565-4d41-90e9-984c3f9f279e",
   "metadata": {},
   "outputs": [],
   "source": [
    "muon_pt_tensor[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009c0de6-6faa-4296-a6db-91a2d8723564",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92736cd-d141-46f3-b23a-8444b043cac5",
   "metadata": {},
   "source": [
    "Unfortunately, one of the few things that you can do with it is turn it into a regular array by padding. (See [ak.pad_none](https://awkward-array.org/doc/main/reference/generated/ak.pad_none.html) and [ak.fill_none](https://awkward-array.org/doc/main/reference/generated/ak.fill_none.html) in Awkward Array.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adf85f6-da2f-421d-a4a0-dc42382f3699",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nested.to_padded_tensor(muon_pt_tensor, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90898514-8945-4278-b917-cabd971561db",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5304fd0-9ace-4975-b647-5eef53cd706e",
   "metadata": {},
   "source": [
    "Other than computational inefficiency (iterating in Python, padded arrays in memory, extra-wide neural network layers to transform padded arrays), there are problems with data like this from a machine learning point of view.\n",
    "\n",
    "A network trained on the padded tensor above would learn that many of the values on the right are `-1`, and it would learn _the exact order_ of muon values (which might or might not be sorted).\n",
    "\n",
    "We want a model to learn about the muons as _unsorted collections of objects_.\n",
    "\n",
    "We want **permutation invariance**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6952e72-cf91-4886-91eb-78713064411f",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c573a86-2c2f-4cd0-b921-52a4c42f930a",
   "metadata": {},
   "source": [
    "[Zaheer, Kottur, Ravanbakhsh, Poczos, Salakhutdinov, & Smola, _Deep Sets_](https://arxiv.org/abs/1703.06114) (2017):\n",
    "\n",
    "<img src=\"../img/deepset-theorem.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d460d8-89bf-4a6d-b935-9e8cbb2d157f",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8750bd94-6eca-45bb-990b-75b8bce69487",
   "metadata": {},
   "source": [
    "That is, to approximate a function $f(x_1, x_2, \\ldots, x_M)$ with no dependence on the order of $x_1$, $x_2$, ... $x_M$, you can transform each $x_i$ into an independent vector, sum them, and then transform that vector.\n",
    "\n",
    "These two transformations, $\\Phi$ and $F$, can be neural networks, for complete generality.\n",
    "\n",
    "$$ f(x_1, x_2, \\ldots, x_M) = F \\left( \\sum_{i = 1}^M \\Phi(x_i) \\right) $$\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"../img/deepset-diagram.png\" width=\"500\"></p>\n",
    "\n",
    "The $\\Phi$ functions should expand the $x_i$ vectors to a larger space so that enough information is preserved when they're summed.\n",
    "\n",
    "The larger the typical number of $x_i$ (e.g. modeling all tracks, rather than just muons), the larger the output dimensionality of $\\Phi$ should be.\n",
    "\n",
    "The **latent space** (above) is the same kind of embedding that we saw in the pinch point of the autoencoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5d7015-f73e-48fa-b50f-d1c6d1e3ab1b",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f0f743-0a90-4d86-9022-0b5426e6de0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "muon_kinematics = event_data[\"muon\", [\"pt\", \"eta\", \"phi\"]]\n",
    "\n",
    "muon_kinematics_tensor = torch.tensor(\n",
    "    ak.to_numpy(ak.flatten(muon_kinematics)).view(np.float32).reshape(-1, 3)\n",
    ")\n",
    "muon_kinematics_tensor[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcd30c8-9f99-44a0-aaeb-eb05d8316f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_data[0, \"muon\", [\"pt\", \"eta\", \"phi\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0884a1-8682-4275-92a2-f8986873b746",
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi = nn.Sequential(\n",
    "    nn.Linear(3, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 10),\n",
    ")\n",
    "\n",
    "F = nn.Sequential(\n",
    "    nn.Linear(10, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 10),\n",
    "    nn.ReLU(),\n",
    ")\n",
    "\n",
    "start = 0\n",
    "for i, count in enumerate(ak.num(muon_kinematics)):\n",
    "    one_event = muon_kinematics_tensor[start : start + count]\n",
    "    start += count\n",
    "\n",
    "    prediction = F(torch.sum(Phi(one_event), axis=0, keepdims=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d722c058-6ac0-4cab-bbbc-cfbc060af45d",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc7a3e7-5a39-4145-9325-d2f3a8aed795",
   "metadata": {},
   "source": [
    "The growing list of vector-transformations that can be used in neural networks:\n",
    "\n",
    "1. linear transformations\n",
    "2. non-linear activation functions\n",
    "3. random generation from distribution parameters\n",
    "4. small set of learned convolution parameters applied to large images\n",
    "5. reducing an image size with pooling\n",
    "6. adding arbitrarily many neural network outputs to make the next neural network input.\n",
    "\n",
    "Moving on!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35218d8-c7be-48ff-9452-c8b641bc586b",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aa8517-d445-4056-b77c-326f7b6a6514",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Graph neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989da2a8-3759-491a-8a04-b522970712e5",
   "metadata": {},
   "source": [
    "What's a graph?\n",
    "\n",
    "<img src=\"../img/13129_2024_67_Fig5_HTML.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1bb90a-237e-4f8a-995a-d7f3e3d09c89",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690a4960-e9c3-4e23-bb48-10cacbd09a5c",
   "metadata": {},
   "source": [
    "A graph consists of distinct nodes (points) connected by edges (lines), in which only the connections matter, not where they're located/how they're drawn on a page.\n",
    "\n",
    "* Nodes may have properties, such as a label.\n",
    "* Edges may have properties, such as directions and weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2f47ee-a558-4d21-8519-2659bcf09256",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34940fb-ead0-4fbe-97f9-c78e9c597b79",
   "metadata": {},
   "source": [
    "What's the difference between a set\n",
    "\n",
    "<img src=\"../img/example-set.svg\" width=\"500\">\n",
    "\n",
    "<br><br>\n",
    "\n",
    "and a graph?\n",
    "\n",
    "<img src=\"../img/example-graph.svg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf7ee06-73b4-49d2-bb3d-aa47aecaf6a8",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7f2956-e1f8-4dc1-a119-5905838d744f",
   "metadata": {},
   "source": [
    "A graph is a set with edges.\n",
    "\n",
    "Graphs should have the same **permutation invariance** as sets.\n",
    "\n",
    "We can model graph data in a way that is similar to DeepSets by adding an extra step that handles the edges.\n",
    "\n",
    "Instead of adding $\\Phi(x_i)$ for all nodes $x_i$ equally, as in DeepSets, **Graph Neural Networks** (GNNs) sum over individual neighborhoods (in various ways).\n",
    "\n",
    "<img src=\"../img/graph-neighborhoods.svg\" width=\"500\">\n",
    "\n",
    "[Ward, Joyner, Lickfold, Guo, & Bennamoun, _A Practical Tutorial on Graph Neural Networks_](https://arxiv.org/abs/2010.05234) (2020).\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<img src=\"../img/gnn-equation.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12afc1f-23ba-4e3d-8860-dcd68617f911",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b46c4c1-a6e5-4ef6-bd82-b7b79a8715f2",
   "metadata": {},
   "source": [
    "The design space for GNNs is huge, but they involve using ordinary neural networks to make **latent spaces** ([a.k.a. embedding spaces](https://ai.stackexchange.com/q/11285)) and summing (or maximizing) over edge-connected neighbors in the graph.\n",
    "\n",
    "Papers and webpages are filled with diagrams and equations with lots of subscripts that try to express this connectivity.\n",
    "\n",
    "(See [the Neural Network Zoo](https://www.asimovinstitute.org/neural-network-zoo/).)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac7b4b0-cd38-4b17-aead-6e8292b8be6d",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a7d6af-9980-4843-a3ac-3076d3f9fbf8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Transformers (such as ChatGPT)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a702e66b-2ba3-476a-a887-08d4eceb5374",
   "metadata": {},
   "source": [
    "You'll see this diagram everywhere—it's called a **transformer** architecture:\n",
    "\n",
    "<img src=\"../img/transformer-architecture.svg\" width=\"400\">\n",
    "\n",
    "[Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, & Polosukhin, _Attention Is All You Need_](https://arxiv.org/abs/1706.03762) (2017)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edda83a-6caf-4b8d-b78a-640a53eaac62",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab428de-5825-4060-a8d1-75fff6fcf8e2",
   "metadata": {},
   "source": [
    "Although the most famous application of the transformer architecture is ChatGPT and other Large Language Models (LLMs), it is a generalization of GNNs and is likely applicable to HEP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95175065-bb53-4977-a414-ce207f9e754c",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "16d72f7b-fa1e-4510-a2fe-264a9aead5ef",
   "metadata": {},
   "source": [
    "The key part of the transformer architecture is **attention**. (Hence the paper title, \"Attention is all you need.\")\n",
    "\n",
    "Attention is a dynamic weight between all pairs of inputs, learned in the same optimization with the data themselves.\n",
    "\n",
    "It was developed in the context of human language translation ([Bahdanau, Cho, & Bengio, _Neural Machine Translation by Jointly Learning to Align and Translate_](https://arxiv.org/abs/1409.0473) (2014)):\n",
    "\n",
    "<p style=\"margin-top: -50px;\"><img src=\"../img/real-cross-attention-a.svg\" width=\"400\"><img src=\"../img/real-cross-attention-d.svg\" width=\"400\"></p>\n",
    "\n",
    "> la zone économique européenne\n",
    "\n",
    "> the European Economic Area\n",
    "\n",
    "<br><br>\n",
    "\n",
    "Or more dramatically:\n",
    "\n",
    "<img src=\"../img/cross-attention-in-french.svg\" width=\"800\">\n",
    "\n",
    "<img src=\"../img/cross-attention-in-hindi.svg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd0b4d8-c231-40d8-9414-f258c1a53392",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7a161d-c58a-4efe-b226-a0287657361d",
   "metadata": {},
   "source": [
    "Autocomplete engines (like ChatGPT) use an attention distribution between a sentence and itself: **self-attention** instead of **cross-attention**.\n",
    "\n",
    "Notice that \"it\" maps to the two nouns in the sentence, but more strongly to \"animal.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c9a7a8-e54c-4b7a-a493-34dc31250537",
   "metadata": {},
   "source": [
    "<img src=\"../img/self-attention.gif\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d503e1d0-ace4-44f2-b971-a5939f8a616d",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2408c4-b067-4f64-9a07-2f2d91043d54",
   "metadata": {},
   "source": [
    "Attention mechanisms appear in three places in the transformer architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea44bf38-f960-4ee7-9098-9106820bc572",
   "metadata": {},
   "source": [
    "<img src=\"../img/transformer-architecture-attention.svg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfb9f31-3fbc-4efe-94e7-77a88fd9a8b8",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f8f089-2713-4097-ab4e-3b912afeacd9",
   "metadata": {},
   "source": [
    "\"Multi-head\" attention is a concatenation of $N$ attention results.\n",
    "\n",
    "<img src=\"../img/multi-head-attention_l1A3G7a.png\" width=\"300\">\n",
    "\n",
    "It adds one more type of vector manipulation to the list:\n",
    "\n",
    "1. linear transformations\n",
    "2. non-linear activation functions\n",
    "3. random generation from distribution parameters\n",
    "4. small set of learned convolution parameters applied to large images\n",
    "5. reducing an image size with pooling\n",
    "6. adding arbitrarily many neural network outputs to make the next neural network input\n",
    "7. concatenate vectors (make a $n_1 + n_2$-dimensional space from $n_1$-dimensional and $n_2$-dimensional spaces).\n",
    "\n",
    "Basically, any array-oriented manipulation (that you can differentiate through, to help the optimizer) is fair game."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0bbd43-0081-46dc-b01f-d4edae3fa61c",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d4b4bd-698d-4583-acaa-f5c52935bba0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Closing remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c7aa6a-8036-4f41-a0cf-1802631d723b",
   "metadata": {},
   "source": [
    "<img src=\"../img/craftsmanship.jpg\" width=\"400\" style=\"margin-right: 30px;\"><img src=\"../img/farming.jpg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ac1096-453c-4552-80ae-d357d1f7a3d5",
   "metadata": {},
   "source": [
    "Machine learning doesn't magically take away all the difficulty (or interestingness!) of programming: it replaces one set of issues with another.\n",
    "\n",
    "| traditional programming (craftsmanship) | machine learning (farming) |\n",
    "|:--:|:--:|\n",
    "| type correctness | defining loss functions |\n",
    "| mutable state | tweaking optimizers, batch sizes |\n",
    "| data structures, algorithms | under & overfitting |\n",
    "| modularization, separation of concerns | regularization |\n",
    "| API design | training, validation, testing |\n",
    "| concurrency | deciding what is a good fit |\n",
    "| memory management | designing the architecture |\n",
    "| ... | ... |\n",
    "\n",
    "_And_ some problems are better suited to traditional programming, while others are better suited to machine learning. With traditional programming, you can determine what the program does exactly. With machine learning, you can grow more complex systems than a human mind could ever develop.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "Final thoughts: [Andrej Karpathy, _A Recipe for Training Neural Networks_](https://karpathy.github.io/2019/04/25/recipe/) (2019)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
