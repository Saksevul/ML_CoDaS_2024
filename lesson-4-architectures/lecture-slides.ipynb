{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4cda695-e7bb-402d-847c-af5d9092c84b",
   "metadata": {},
   "source": [
    "# Machine Learning at CoDaS-HEP 2024, Lesson 4: Survey of Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c63bb9d-ce3a-474f-acf7-de7c2bd95925",
   "metadata": {},
   "source": [
    "In lesson 1, I introduced neural networks and the universal function approximation theorem. A single hidden layer implements _adaptive_ basis functions, more flexible than classic Taylor and Fourier series.\n",
    "\n",
    "In lesson 2, we talked about issues involed in any fitting procedure, whether multilayered or not (i.e. a pure linear fit).\n",
    "\n",
    "Lesson 3 was an open-ended project to build your own neural network.\n",
    "\n",
    "In lesson 4, we will consider a variety of neural network \"architectures\": ways of building networks to improve learning for different types of problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89a9945-0844-4119-b452-6bedbab69fa0",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd9c33a-086e-4711-bfc8-ba8d2ba3dbae",
   "metadata": {},
   "source": [
    "## Why should learning be \"deep\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7ce9d2-f8c5-474d-8cae-a0a7e007bb41",
   "metadata": {},
   "source": [
    "**Deep learning:** a neural network with 3 or more layers.\n",
    "\n",
    "<img src=\"../img/rise-of-deep-learning.svg\" width=\"800\">\n",
    "\n",
    "* 2006‒2007: solved problems in training algorithms that _prevented_ deep learning.\n",
    "* 2012: AlexNet, a GPU-enabled 8 layer network with ReLU, won the ImageNet competition.\n",
    "* 2015: ResNet, a GPU-enabled 152+ layer network with skip-connections, won the ImageNet competition.\n",
    "\n",
    "By 2015, it was clear that networks with many layers have more potential than one big hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63ab18f-6916-40cf-8c04-c2b7598f6161",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ce597b-9a7e-44aa-b98a-e24e76e4d724",
   "metadata": {},
   "source": [
    "Why does it work?\n",
    "\n",
    "One big hidden layer can approximate any shape, by optimizing adaptive basis functions, but\n",
    "\n",
    "> Shallow networks are very good at memorization, but not so good at generalization.\n",
    "\n",
    "That is, they have a tendency to overfit.\n",
    "\n",
    "Why are multiple layers better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24745c8f-8b38-469a-87b1-6a2ba9e5c56a",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faef60c-10ce-4dd9-9c09-1dcbcee9a6d1",
   "metadata": {},
   "source": [
    "Adaptive basis function:\n",
    "\n",
    "$$ \\psi(x; a, b) = \\left\\{\\begin{array}{c l}\n",
    "a + b x & \\mbox{if } x > -a/b \\\\\n",
    "0 & \\mbox{otherwise} \\\\\n",
    "\\end{array}\\right. $$\n",
    "\n",
    "Function approximation with one hidden layer:\n",
    "\n",
    "$$ f_j(x) = \\sum_i^{N_1} \\psi(x; a_{ij}, b_{ij}) c_{ij} $$\n",
    "\n",
    "Function approximation with two hidden layers:\n",
    "\n",
    "$$ f_k(x) = \\sum_j^{N_2} \\psi\\left(x; \\left[\n",
    "\\sum_i^{N_1} \\psi(x; a_{i1}, b_{i1}) c_{i1}\n",
    "\\right], \\left[\n",
    "\\sum_i^{N_1} \\psi(x; a_{i2}, b_{i2}) c_{i2}\n",
    "\\right]\\right) \\left[\n",
    "\\sum_i^{N_1} \\psi(x; a_{i3}, b_{i3}) c_{i3}\n",
    "\\right] $$\n",
    "\n",
    "And so on: adaptively adaptive basis functions, then adaptively adaptively adaptive basis functions..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d135cb58-4ae9-4197-898f-88746d66a86f",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3234da30-09e3-4683-8c67-4235b112930e",
   "metadata": {},
   "source": [
    "* More neurons in a single layer adds wiggles to a fit function.\n",
    "* More layers effectively fold the space in which a wiggly function can fit the data. Instead of finding individual wiggles, they find symmetries in the data that (probably) correspond to the underlying relationship, rather than noise.\n",
    "\n",
    "Consider this horseshoe-shaped decision boundary: with two well-chosen folds along the symmetries, it reduces to a simpler curve to fit. Instead of 4 ad-hoc wiggles, it's 2 folds and 1 wiggle.\n",
    "\n",
    "<img src=\"../img/deep-learning-by-space-folding.svg\" width=\"800\">\n",
    "\n",
    "Montúfar, Pascanu, Cho, & Bengio, [_On the Number of Linear Regions of Deep Neural Networks_](https://arxiv.org/abs/1402.1869) (2014)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70914eca-bf0f-4858-884f-d8d9d35586e8",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4339d9bb-5af1-47c2-8274-563233e22acc",
   "metadata": {},
   "source": [
    "Roy Keyes's fantastic demo ([with code](https://gist.github.com/jpivarski/f99371614ecaa48ace90a6025d430247)):\n",
    "\n",
    "<img src=\"../img/network-layer-space-folding.png\" width=\"800\">\n",
    "\n",
    "A uniform grid on the feature space (left; grid not shown) projected through the first layer's transformation shows what the underlying space looks like (right; grid is gray) before the second layer makes a linear decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43466ae2-3dc7-4f33-ae63-f2aa5970e7ef",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c208726-9aec-48dc-8dba-5b7fc9ae7e3a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "This is our first architecture:\n",
    "\n",
    "<img src=\"../img/artificial-neural-network-layers-2.svg\" width=\"700\">\n",
    "\n",
    "Neurons in a layer add wiggles to the fitted function; layers add reflections and symmetries that are (probably) real structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd428bb-ddac-4ad7-9dfd-2390d6536785",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abed1d9-e14d-4a65-9e98-a9c19cc7b8c2",
   "metadata": {},
   "source": [
    "## Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93f3c63e-1d3c-40db-9af2-59cab7034f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn.datasets\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ffd4e25-2d1e-4daf-903d-250c7310de42",
   "metadata": {},
   "outputs": [],
   "source": [
    "hls4ml_lhc_jets_hlf = sklearn.datasets.fetch_openml(\"hls4ml_lhc_jets_hlf\")\n",
    "\n",
    "features = torch.tensor(hls4ml_lhc_jets_hlf[\"data\"].values).float()\n",
    "targets = torch.tensor(hls4ml_lhc_jets_hlf[\"target\"].cat.codes.values).long()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
