{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16f240ed-4887-4955-b015-51be367377e1",
   "metadata": {},
   "source": [
    "# Issues in practice: lecture slides"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb77b630-053f-4b00-a60f-be62554cac66",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b4d7d8-3a44-461f-be35-54dcaabc0eb5",
   "metadata": {},
   "source": [
    "## Which library to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7d467d-d23b-4bf5-a830-d61633323988",
   "metadata": {},
   "source": [
    "**Many (canned) algorithms:**\n",
    "\n",
    "<table>\n",
    "    <tr style=\"background: white;\">\n",
    "        <td><img src=\"https://upload.wikimedia.org/wikipedia/commons/0/05/Scikit_learn_logo_small.svg\" style=\"height: 90px;\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "* Scikit-Learn's algorithms have a common interface, which makes it easy to learn a new one, but they're not flexible (only configurable by a long list of function arguments).\n",
    "\n",
    "<br><br>\n",
    "\n",
    "**Neural networks:**\n",
    "\n",
    "<table>\n",
    "    <tr style=\"background: white;\">\n",
    "        <td style=\"text-align: center; padding-right: 15px;\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/a/ab/TensorFlow_logo.svg\" style=\"height: 140px;\"></td>\n",
    "        <td style=\"text-align: center; padding-left: 15px;\"><img src=\"https://keras.io/img/logo.png\" style=\"height: 60px;\"></td>\n",
    "    </tr> <tr style=\"background: white;\">\n",
    "        <td style=\"text-align: center; padding-right: 15px;\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/c/c6/PyTorch_logo_black.svg\" style=\"height: 45px;\"></td>\n",
    "        <td style=\"text-align: center; padding-left: 15px;\"><img src=\"https://raw.githubusercontent.com/valohai/ml-logos/master/mxnet.svg\" style=\"height: 60px;\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "* TensorFlow and Keras merged, with Keras becoming the high-level interface to TensorFlow.\n",
    "* TensorFlow tends to be associated more with industry and production environments; PyTorch tends to be associated more with academic research.\n",
    "  * In PyTorch, the training loop (feeding the algorithm batches of data to fit) is just a Python for loop, which makes it easy to develop and debug.\n",
    "  * In TensorFlow 2.0 (2019) and later, training can be done this way as well.\n",
    "  * Both can be hard to install if you're trying to use a GPU (hard to match your CUDA library version).\n",
    "* MXNet is much less popular than TensorFlow and PyTorch, and development ended recently (2023).\n",
    "\n",
    "<br><br>\n",
    "\n",
    "**Neural network-capable array library:**\n",
    "\n",
    "<table>\n",
    "    <tr style=\"background: white;\">\n",
    "        <td><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/86/Google_JAX_logo.svg\" style=\"height: 75px;\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "* JAX is a NumPy replacement with support for GPU, autodiff, and JIT-compilation, which makes it possible to build new algorithms from scratch.\n",
    "  * Same installation troubles with GPUs/CUDA.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "**Boosted decision trees:**\n",
    "\n",
    "<table>\n",
    "    <tr style=\"background: white;\">\n",
    "        <td style=\"text-align: center; padding-right: 30px; padding-bottom: 25px;\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/6/69/XGBoost_logo.png\" style=\"height: 60px;\"></td>\n",
    "        <td style=\"text-align: center;\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/d/d9/LightGBM_logo_black_text.svg\" style=\"height: 45px;\"></td>\n",
    "        <td style=\"text-align: center; padding-left: 30px;\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/c/cc/CatBoostLogo.png\" style=\"height: 70px; vertical-align: middle; padding-bottom: 15px;\"> <span style=\"font-size: 18pt;\">CatBoost</span></td>\n",
    "</table>\n",
    "\n",
    "* Boosted decision trees are still relevant for problems with well-chosen input features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e9e185-30de-4145-8841-a2ce384e7e22",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef2bcce-62bd-4a57-94d9-5ce113362d3a",
   "metadata": {},
   "source": [
    "### What does everyone else use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08ffa03-3e41-4055-898a-b17fa501ad02",
   "metadata": {},
   "source": [
    "Using techniques described [here](https://indico.jlab.org/event/459/contributions/11547/) and [here](https://github.com/jpivarski-talks/2023-05-09-chep23-analysis-of-physicists), this is the number of times each ML library is imported, semiannually, in code written by CMS physicists:\n",
    "\n",
    "<img src=\"../img/github-ml-package-cmsswseed.svg\" width=\"900\">\n",
    "\n",
    "* Scikit-Learn is oldest and still widely used\n",
    "* TensorFlow is dominant (and used to be imported with Keras, but not so much anymore)\n",
    "* PyTorch is increasingly significant\n",
    "* XGBoost is also common\n",
    "* JAX and the other libraries are not widely used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97a2aa5-a214-43a2-b7b1-8c8151e329a2",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57375b34-290c-4ecb-8667-3b4bdb0412dc",
   "metadata": {},
   "source": [
    "Using Google Trends (world search volume, not just physicists):\n",
    "\n",
    "<img src=\"../img/google-ml-package.svg\" width=\"900\">\n",
    "\n",
    "* PyTorch saw a burst of interest since 2023, but it's because of LLMs:\n",
    "\n",
    "<img src=\"../img/google-ml-llm-package.svg\" width=\"900\">\n",
    "\n",
    "* That's not relevant for HEP, though it would be interesting to extend my analysis of CMS physicsts one more year."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c797f4-139e-4e53-b1bd-7382e6c97d9f",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f2c69e-e7de-4d1f-9b36-501129e3e426",
   "metadata": {},
   "source": [
    "### What will this mini-course use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee7118a-c98f-4a8b-b8b1-e4e0a06bed62",
   "metadata": {},
   "source": [
    "Scikit-Learn for linear fits and simple neural networks.\n",
    "\n",
    "PyTorch for neural network architectures, because it's easier to illustrate the parts.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "Most of the examples in _this lesson_ are linear fits because the issues that I'll be discussing are general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339d0638-ac03-4c29-829c-6eb1c97607a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mpl_toolkits.mplot3d\n",
    "\n",
    "import sklearn.linear_model\n",
    "import torch\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256c3827-68b2-4046-9b58-4afcb5b3cf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins_df = pd.read_csv(\"../data/penguins.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334f9c56-e463-44cc-8165-3e0e49dbf3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_prices_df = pd.read_csv(\n",
    "    \"../data/boston-house-prices.csv\", sep=\"\\s+\", header=None,\n",
    "    names=[\"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \"RM\", \"AGE\", \"DIS\", \"RAD\", \"TAX\", \"PTRATIO\", \"B\", \"LSTAT\", \"MEDV\"],\n",
    ")\n",
    "boston_prices_df = (boston_prices_df - boston_prices_df.mean()) / boston_prices_df.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a472146e-babf-4f12-b05c-62aae1272357",
   "metadata": {},
   "outputs": [],
   "source": [
    "stockmarket_data = [float(x) for x in open(\"../data/nasdaq-NXPI-20160726-to-20170428.csv\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd1b4c0-5f39-42a2-98e3-1e3be673007a",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cae10c1-b2ce-44c3-a7db-a924c737d3bc",
   "metadata": {},
   "source": [
    "## Regression versus classification, loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af8c041-47ae-4192-9aec-748654150a97",
   "metadata": {},
   "source": [
    "When we think of fitting, we usually think of regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6248a9b-1719-42ae-bcb1-b9d57a8a1280",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sklearn.linear_model.LinearRegression()\n",
    "model.fit(penguins_df.dropna()[\"flipper_length_mm\"].values[:, np.newaxis], penguins_df.dropna()[\"body_mass_g\"])\n",
    "\n",
    "model_x = np.linspace(170, 240, 2)\n",
    "model_y = model.predict(model_x[:, np.newaxis])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "penguins_df.plot.scatter(\"flipper_length_mm\", \"body_mass_g\", ax=ax)\n",
    "ax.plot(model_x, model_y, color=\"orange\", linewidth=3)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898aea1b-e80f-4bcf-8ca1-9387f83c737d",
   "metadata": {},
   "source": [
    "Given $x$ values drawn from a vector space, the model responds with the most likely $y$ values, which also come from a vector space (that is, they're real-valued, maybe more than one of them)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cad44c-1f2d-4980-93e3-f8cb51ddd27e",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cff40b3-7e8e-4dad-9427-87e633dc7b73",
   "metadata": {},
   "source": [
    "Sometimes, though, you're interested in _categorical_ data, like the colors of the dots in the TensorFlow Playground exercise.\n",
    "\n",
    "In general, there are [four levels of measurement](https://en.wikipedia.org/wiki/Level_of_measurement),\n",
    "\n",
    "| Level | Math | Description | Physics example |\n",
    "|:--|:--:|:--|:--|\n",
    "| Nominal category | =, ≠ | categories without order | jet classification, data versus Monte Carlo |\n",
    "| Ordinal category | >, < | categories that have an order | barrel region, overlap region, endcap region |\n",
    "| Interval number | +, ‒ | doesn't have an origin | energy, voltage, position, momentum |\n",
    "| Ratio number | ×, / | has an origin | absolute temperature, mass, opening angle |\n",
    "\n",
    "But generally, we only need to be concerned about categorical versus numerical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9f021c-3ecb-4e17-b07e-e23cf6b6f6bf",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0942e3e9-c076-46cd-b1c5-2714fe1abe54",
   "metadata": {},
   "source": [
    "Suppose we have data from two distinct categories that have different, but overlapping, distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe26747-2f2f-4de8-8a50-e245c27a4736",
   "metadata": {},
   "outputs": [],
   "source": [
    "adelie_df = penguins_df[penguins_df[\"species\"] == \"Adelie\"][[\"bill_length_mm\", \"bill_depth_mm\"]].dropna()\n",
    "gentoo_df = penguins_df[penguins_df[\"species\"] == \"Gentoo\"][[\"bill_length_mm\", \"bill_depth_mm\"]].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a337eb6-f11d-48e3-92a9-3214c12074ec",
   "metadata": {},
   "source": [
    "<img src=\"../img/culmen_depth.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333ad717-7e08-4c46-bc8d-74175b1d1f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "adelie_df[\"bill_depth_mm\"].plot.hist(alpha=0.5)\n",
    "gentoo_df[\"bill_depth_mm\"].plot.hist(alpha=0.5)\n",
    "\n",
    "ax.set_xlabel(\"Bill depth (mm)\")\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785e8c61-76fa-41f8-b171-1c4846b7d9d9",
   "metadata": {},
   "source": [
    "We can express the two categories as values 0 and 1 in a numeric dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1584db50-d4d4-402e-847b-6875a68f57fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "species = np.concatenate([np.zeros(len(adelie_df)), np.ones(len(gentoo_df))])\n",
    "bill_depth = np.concatenate([adelie_df[\"bill_depth_mm\"].values, gentoo_df[\"bill_depth_mm\"].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4533079-72ea-4e3f-8c94-0ab7490ac959",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(species, bill_depth, marker=\"x\")\n",
    "\n",
    "ax.set_xlabel(\"Species; 0 = Adelie, 1 = Gentoo\")\n",
    "ax.set_ylabel(\"Bill depth (mm)\")\n",
    "ax.set_xlim(-0.5, 1.5)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48edb93-fbe8-4f59-9248-e624e9f1c17e",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3c85dd-5f41-4061-b705-54213e3d5930",
   "metadata": {},
   "source": [
    "### Categorical → numerical and numerical → categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b7a36c-34ab-493d-8046-b7f9f38c7f76",
   "metadata": {},
   "source": [
    "If we are intending to use the categorical data as a _feature_, an input to the model, then we can fit it as-is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851d87ef-21c6-4985-b755-4d99ee027139",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sklearn.linear_model.LinearRegression()\n",
    "model.fit(species[:, np.newaxis], bill_depth)\n",
    "\n",
    "model_x = np.linspace(-0.5, 1.5, 2)\n",
    "model_y = model.predict(model_x[:, np.newaxis])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(species, bill_depth, marker=\"x\")\n",
    "ax.plot(model_x, model_y, color=\"orange\", linewidth=3)\n",
    "\n",
    "ax.set_xlabel(\"Species; 0 = Adelie, 1 = Gentoo\")\n",
    "ax.set_ylabel(\"Bill depth (mm)\")\n",
    "ax.set_xlim(-0.5, 1.5)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac7f932-a437-4558-bbfd-721ddb3791e6",
   "metadata": {},
   "source": [
    "The fit value at 0 is the average Adelie bill depth and the fit value at 1 is the average Gentoo bill depth.\n",
    "\n",
    "But if the model is supposed to _predict_ the categorical data, as an output of the model, then a linear fit is not meaningful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d85e53f-b9e2-43f9-b6a2-2c0f248efc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sklearn.linear_model.LinearRegression()\n",
    "model.fit(bill_depth[:, np.newaxis], species)\n",
    "\n",
    "model_x = np.linspace(12, 22, 2)\n",
    "model_y = model.predict(model_x[:, np.newaxis])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(bill_depth, species, marker=\"x\")\n",
    "ax.plot(model_x, model_y, color=\"orange\", linewidth=3)\n",
    "\n",
    "ax.set_xlabel(\"Bill depth (mm)\")\n",
    "ax.set_ylabel(\"Species; 0 = Adelie, 1 = Gentoo\")\n",
    "\n",
    "ax.text(18, 0.5, \"WRONG!!!\", color=\"red\", size=22)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9760f164-e93d-4b8d-b79d-7a7ddf4bb3df",
   "metadata": {},
   "source": [
    "Whether the model is predicting an allowed species value of 0 or 1, an intermediate value like 0.5, or an out-of-bounds value like 1.5, it's not expressing anything that means anything to us.\n",
    "\n",
    "Since a linear fit or neural network has to predict continuous numerical values, we can make them useful and interpret them as probabilities. In the training dataset, \"0 = Adelie, 1 = Gentoo\" can be interpreted as the probability of Gentoo.\n",
    "\n",
    "The sigmoid function from the previous lecture,\n",
    "\n",
    "$$ p(x) = \\frac{1}{1 + \\exp(x)} $$\n",
    "\n",
    "clamps the output value of the model between 0 and 1, and a loss function of\n",
    "\n",
    "<!--\n",
    "$$ \\mathcal{L}_k(x_i) = \\left\\{\\begin{array}{l l}\n",
    "-\\log (p_k(x_i)) & \\mbox{if species of } x_i \\mbox{ is } k \\\\\n",
    "-\\log (1 - p_k(x_i)) & \\mbox{if species of } x_i \\mbox{ is not } k \\\\\n",
    "\\end{array}\\right. $$\n",
    "-->\n",
    "\n",
    "$$ \\mbox{loss}(x_i, y_i) = -(y_i) \\log \\big[ p(x_i) \\big] - (1 - y_i) \\log \\big[ 1 - p(x_i) \\big] $$\n",
    "\n",
    "optimizes a linear model to predict probabilities $p(x_i)$ that match the given probabilities $y_i$.\n",
    "\n",
    "In Scikit-Learn, this is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67f3830-0178-4aca-a399-1def0fff44f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sklearn.linear_model.LogisticRegression(penalty=None)\n",
    "model.fit(bill_depth[:, np.newaxis], species)\n",
    "\n",
    "model_x = np.linspace(12, 22, 1000)\n",
    "model_y = model.predict_proba(model_x[:, np.newaxis])[:, 1]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(bill_depth, species, marker=\"x\")\n",
    "ax.plot(model_x, model_y, color=\"orange\", linewidth=3)\n",
    "\n",
    "ax.set_xlabel(\"Bill depth (mm)\")\n",
    "ax.set_ylabel(\"Probability that species is Gentoo\")\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7574df-6bc2-44f1-9ffe-242d3fd09493",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ba550b-a8ea-458d-9d67-386fa2da1f92",
   "metadata": {},
   "source": [
    "### Classification in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba4a084-fa50-4925-8ebb-8ddff9da9517",
   "metadata": {},
   "source": [
    "2 lines of code in Scikit-Learn becomes a class definition, 8 lines of initialization, and an explicit 5-line loop over training steps in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6719e9-27b3-4b5b-9e47-a7622780b54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this user-defined model step centers and scales the input\n",
    "# without this, the optimizer would require many more steps to find the minimum\n",
    "class NormalizeInput(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"mean\", torch.tensor([mean], dtype=torch.float32))\n",
    "        self.register_buffer(\"std\", torch.tensor([std], dtype=torch.float32))\n",
    "    def forward(self, x):\n",
    "        return (x - self.mean) / self.std\n",
    "\n",
    "normalize_input = NormalizeInput(bill_depth.mean(), bill_depth.std())\n",
    "\n",
    "model = nn.Sequential(      # define a 3-step model\n",
    "    normalize_input,        # step 1: center/scale the input\n",
    "    nn.Linear(1, 1),        # step 2: linear transformation (1D → 1D)\n",
    "    nn.Sigmoid(),           # step 3: pass output through a sigmoid\n",
    ")\n",
    "\n",
    "# convert the data into PyTorch Tensors, which are differentiable and can live on a GPU\n",
    "features = torch.tensor(bill_depth[:, np.newaxis], dtype=torch.float32)\n",
    "targets = torch.tensor(species[:, np.newaxis], dtype=torch.float32)\n",
    "\n",
    "# use Binary Cross Entropy as a loss function\n",
    "loss_function = nn.BCELoss()\n",
    "\n",
    "# use Adam as an optimizer with a (high) learning rate of 0.03\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.03)\n",
    "\n",
    "# iterate over the same data 1000 times (epochs)\n",
    "for epoch in range(1000):\n",
    "    # tell the optimizer to begin an optimization step\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # give the model the input features and ask it to compute its predictions\n",
    "    predictions = model(features)\n",
    "\n",
    "    # compute the loss between these predictions and the intended targets\n",
    "    loss = loss_function(predictions, targets)\n",
    "\n",
    "    # tell the loss function and optimizer to end an optimization step\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49ae6e5-de83-461f-9f94-210243d95ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_x = np.linspace(12, 22, 1000)\n",
    "model_y = model(torch.tensor(model_x[:, np.newaxis], dtype=torch.float32)).detach().numpy()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(bill_depth, species, marker=\"x\")\n",
    "ax.plot(model_x, model_y, color=\"orange\", linewidth=3)\n",
    "\n",
    "ax.set_xlabel(\"Bill depth (mm)\")\n",
    "ax.set_ylabel(\"Probability that species is Gentoo\")\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c37c4d5-e617-4928-ab9f-b8da313bd294",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8afa75c-b6c6-49a4-9378-c9ff8cfa0388",
   "metadata": {},
   "source": [
    "PyTorch makes all of the steps explicit, and that's (eventually) a good thing because we can change any step of the process.\n",
    "\n",
    "The steps that distinguish regression from classification are\n",
    "\n",
    "* including [nn.Sigmoid](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html) as the last step of the `model`\n",
    "* using [nn.BCELoss](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html) instead of [nn.MSELoss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html) as the loss function.\n",
    "\n",
    "See a full list of [PyTorch loss functions here](https://pytorch.org/docs/stable/nn.html#loss-functions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96786f1-32d3-43a2-9372-a57d4226b859",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2046ed-6db0-454b-abb7-462417200728",
   "metadata": {},
   "source": [
    "(The `NormalizeInput` class is an example of a user-defined model step to center and scale the input. Without it, the fit would be much slower. If I pre-processed the data manually, I'd have to remember to apply the same pre-processing on the training and prediction data.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9c1779-d959-45dd-a9da-dc5457a37c9a",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec8f3cb-f7e6-4a20-8ac6-d6b168c29e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601d6db2-27ff-4644-91d5-a06d964851df",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function(torch.tensor([1.0]), torch.tensor([1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8765e084-7128-49bb-8679-4e0940308419",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function(torch.tensor([2.0]), torch.tensor([1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41197b3d-901d-4db2-813e-1eacfae6624f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function(torch.tensor([3.0]), torch.tensor([1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f94ea21-8274-45bd-b8c4-7ff2ec8533fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function(torch.tensor([4.0]), torch.tensor([1.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154bbf47-7491-40f0-9c40-aea07f881131",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45632812-6e4a-4b89-8bde-12d968ffa8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39caaa2c-9f52-430d-88c3-067ad0675a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function(torch.tensor([0.0001]), torch.tensor([1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06108d64-1852-469a-9665-7e67aa1e669a",
   "metadata": {},
   "outputs": [],
   "source": [
    "-np.log(0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb8742f-c888-424f-92d4-2ed9dd950fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function(torch.tensor([0.001]), torch.tensor([1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a651c94d-9bd9-4b7a-bf95-e93a46aa52da",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function(torch.tensor([0.01]), torch.tensor([1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfcc05b-5c17-41ab-a9e2-eddf2466d0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function(torch.tensor([0.1]), torch.tensor([1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ac4a14-abc1-4866-bebc-8492b773353c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function(torch.tensor([0.5]), torch.tensor([1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df62d744-23f5-43d5-b26b-5ade5d7b8eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function(torch.tensor([1.0]), torch.tensor([1.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5995b13-2977-46a5-ba18-2194748dd711",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf3a988-31d6-40ce-84f8-fd7a24b8e999",
   "metadata": {},
   "source": [
    "### More than two classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7522ef-f1c1-4c30-b7da-5366cf66f678",
   "metadata": {},
   "source": [
    "Now suppose that we have more than two classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a7b6d1-c99f-466c-85fe-fbb2d3ca0182",
   "metadata": {},
   "outputs": [],
   "source": [
    "adelie_df = penguins_df[penguins_df[\"species\"] == \"Adelie\"][[\"bill_length_mm\", \"bill_depth_mm\"]].dropna()\n",
    "gentoo_df = penguins_df[penguins_df[\"species\"] == \"Gentoo\"][[\"bill_length_mm\", \"bill_depth_mm\"]].dropna()\n",
    "chinstrap_df = penguins_df[penguins_df[\"species\"] == \"Chinstrap\"][[\"bill_length_mm\", \"bill_depth_mm\"]].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c59fd3d-f778-4f84-a763-189916fe24f3",
   "metadata": {},
   "source": [
    "Can we define a species variable that takes values 0, 1, and 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc381b5-2c34-40ac-9d18-b9ce2f3385dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "adelie_df[\"species\"] = 0\n",
    "gentoo_df[\"species\"] = 1\n",
    "chinstrap_df[\"species\"] = 2\n",
    "\n",
    "ordinal_df = pd.concat([adelie_df, gentoo_df, chinstrap_df])\n",
    "\n",
    "model = sklearn.linear_model.LinearRegression()\n",
    "model.fit(ordinal_df[\"species\"].values[:, np.newaxis], ordinal_df[\"bill_depth_mm\"].values)\n",
    "\n",
    "model_x = np.linspace(-0.5, 2.5, 2)\n",
    "model_y = model.predict(model_x[:, np.newaxis])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ordinal_df.plot.scatter(\"species\", \"bill_depth_mm\", ax=ax)\n",
    "ax.plot(model_x, model_y, color=\"orange\", linewidth=3)\n",
    "\n",
    "ax.set_xlabel(\"Species; 0 = Adelie, 1 = Gentoo, 2 = Chinstrap\")\n",
    "ax.set_ylabel(\"Bill depth (mm)\")\n",
    "\n",
    "ax.text(0.5, 20, \"WRONG!!!\", color=\"red\", size=22)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271fcd8b-5260-45b9-8aae-0edb49a06bc4",
   "metadata": {},
   "source": [
    "Unlike the case with only 2 values for species, the fit is now finding the _order_ and the _distances_ between the values 0, 1, and 2 to be meaningful: it's interpreting species as interval numbers, rather than nominal categories.\n",
    "\n",
    "(With more parameters in the fit, we can wash away that interpretation: for instance, fitting the three categories with a parabola that goes through the mean of each. Neural networks with enough parameters can lose knowledge of the order and distances between integer encodings, so it can be okay in those contexts.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a79754-4b27-4f16-8db4-17ce75c50203",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a46772-c32a-48d9-8dac-9acfcbb1057d",
   "metadata": {},
   "source": [
    "Instead, expand a categorical variable with $n$ categories into $n$ variables whose values are 0 and 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041794b0-7df4-4a1e-8b7e-1e49cd07fa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "adelie_df[\"is_adelie\"] = 1\n",
    "gentoo_df[\"is_gentoo\"] = 1\n",
    "chinstrap_df[\"is_chinstrap\"] = 1\n",
    "\n",
    "onehot_df = pd.concat([adelie_df, gentoo_df, chinstrap_df]).fillna(0)\n",
    "onehot_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dbc9c9-c5f3-45ae-b088-be3d47f2432a",
   "metadata": {},
   "source": [
    "For historical reasons, this is called a [one-hot encoding](https://en.wikipedia.org/wiki/One-hot).\n",
    "\n",
    "The three new features, `is_adelie`, `is_gentoo`, and `is_chinstrap`, can be interpreted as given probabilities, and therefore either 0 and 1. They're constrained to have `is_adelie + is_gentoo + is_chinstrap = 1`. In principle, we only need $n - 1$ variables, since one is determined by the others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011cd6db-71c2-4d3c-9b0a-d25955a7e937",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ba9b5a-325a-44dc-9ac7-d323bf9108c7",
   "metadata": {},
   "source": [
    "Here's a logistic regression in Scikit-Learn, using bill length and bill depth as 2 input features to predict 3 probabilities `is_adelie`, `is_gentoo`, `is_chinstrap`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bc39cf-06cd-4d01-9f65-d83e214cb555",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_np = onehot_df[[\"bill_length_mm\", \"bill_depth_mm\"]].values\n",
    "targets_np = onehot_df[[\"is_adelie\", \"is_gentoo\", \"is_chinstrap\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5fc864-14cf-4c97-94f2-baec33caf26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sklearn.linear_model.LogisticRegression(penalty=None)\n",
    "\n",
    "# Scikit-Learn wants the target index, rather than the probability values themselves, so np.argmax\n",
    "model.fit(features_np, np.argmax(targets_np, axis=1))\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e904dd98-70a6-4df3-a78a-df82afa68fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# draw three scatter plots, each a different color\n",
    "ax.scatter(features_np[targets_np[:, 0] == 1, 0], features_np[targets_np[:, 0] == 1, 1])\n",
    "ax.scatter(features_np[targets_np[:, 1] == 1, 0], features_np[targets_np[:, 1] == 1, 1])\n",
    "ax.scatter(features_np[targets_np[:, 2] == 1, 0], features_np[targets_np[:, 2] == 1, 1])\n",
    "\n",
    "# compute the three probabilities for every 2D point in the background\n",
    "background_x, background_y = np.meshgrid(np.linspace(29, 62, 100), np.linspace(12, 23, 100))\n",
    "background_2d = np.column_stack([background_x.ravel(), background_y.ravel()])\n",
    "probabilities = model.predict_proba(background_2d)\n",
    "\n",
    "# draw contour lines where the probabilities cross the 50% threshold\n",
    "ax.contour(background_x, background_y, probabilities[:, 0].reshape(background_x.shape), [0.5])\n",
    "ax.contour(background_x, background_y, probabilities[:, 1].reshape(background_x.shape), [0.5])\n",
    "ax.contour(background_x, background_y, probabilities[:, 2].reshape(background_x.shape), [0.5])\n",
    "\n",
    "ax.set_xlabel(\"Bill length (mm)\")\n",
    "ax.set_ylabel(\"Bill depth (mm)\")\n",
    "ax.axis([29, 62, 12, 23])\n",
    "\n",
    "ax.legend([\"Adelie\", \"Gentoo\", \"Chinstrap\"])\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadc95a8-70f7-4d60-8269-2a76917e16a6",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fce570-d6c8-4992-a54f-af69d31b16d2",
   "metadata": {},
   "source": [
    "### 5-minute exercise: do a logistic regression of 2 features to 3 categories in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ff3450-72d1-497f-b64d-dfb205ae5b25",
   "metadata": {},
   "source": [
    "Uncomment and run the debugging code if it's helpful to do so.\n",
    "\n",
    "Remember: examine all your variables and look at each step! That's why we're using Python (and Jupyter) in the first place!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01b7b77-62d1-461c-86b1-5da163818b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = torch.tensor(features_np, dtype=torch.float32)\n",
    "targets = torch.tensor(targets_np, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfa438c-d158-49c0-9907-f23935069f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf61109-1f46-4e9b-bd22-add556384385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8f4df4-6a71-4a17-a610-2058e253078c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeInput(nn.Module):\n",
    "    def __init__(self, mean1, std1, mean2, std2):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"mean\", torch.tensor([mean1, mean2], dtype=torch.float32))\n",
    "        self.register_buffer(\"std\", torch.tensor([std1, std2], dtype=torch.float32))\n",
    "    def forward(self, x):\n",
    "        return (x - self.mean) / self.std\n",
    "\n",
    "normalize_input = NormalizeInput(\n",
    "    features_np[:, 0].mean(), features_np[:, 0].std(),\n",
    "    features_np[:, 1].mean(), features_np[:, 1].std(),\n",
    ")\n",
    "# normalize_input.forward(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb48e4f-4ffe-48ff-81b9-a6893200e466",
   "metadata": {},
   "source": [
    "The softmax function is defined as:\n",
    "\n",
    "$$ \\mbox{softmax}_i(x) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)} $$\n",
    "\n",
    "Consequentially,\n",
    "\n",
    "$$ \\sum_i \\mbox{softmax}_i(x) = 1 $$\n",
    "\n",
    "When we had only 2 categories, we could focus on only one of them and compute its sigmoid.\n",
    "\n",
    "With 3 categories, we use the softmax to ensure that the model's output probabilities add to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560f0a3c-42a3-42ad-a675-fe6217c254a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_without_softmax = nn.Sequential(\n",
    "    normalize_input,\n",
    "    nn.Linear(2, 3),\n",
    ")\n",
    "# model_without_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7794e7ab-1d2a-49fb-b7af-b05be54c4b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_without_softmax(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5447dfe5-b4a6-449e-9392-233e25ea64c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_softmax = nn.Sequential(\n",
    "    model_without_softmax,\n",
    "    nn.Softmax(dim=1),\n",
    ")\n",
    "# model_with_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366704b0-d3cc-499a-831d-d8615420cfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_with_softmax(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e63983b-e9a4-416c-a239-65552d2ce864",
   "metadata": {},
   "source": [
    "In the next cell, define a `loss_function`, an `optimizer`, and implement the fit.\n",
    "\n",
    "Use PyTorch's [nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html), which internally includes the softmax. Don't apply it twice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32a51c2-aab4-483c-9d48-d7fc96a22691",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0cf4a3e6-9850-4649-86db-f3991daf76d8",
   "metadata": {},
   "source": [
    "The next cell plots the fit, just like Scikit-Learn, using the model with the softmax (because we need to plot probabilities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a5c358-233b-414c-bd69-b576c280f59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# draw three scatter plots, each a different color\n",
    "ax.scatter(features_np[targets_np[:, 0] == 1, 0], features_np[targets_np[:, 0] == 1, 1])\n",
    "ax.scatter(features_np[targets_np[:, 1] == 1, 0], features_np[targets_np[:, 1] == 1, 1])\n",
    "ax.scatter(features_np[targets_np[:, 2] == 1, 0], features_np[targets_np[:, 2] == 1, 1])\n",
    "\n",
    "# compute the three probabilities for every 2D point in the background\n",
    "background_x, background_y = np.meshgrid(np.linspace(29, 62, 100), np.linspace(12, 23, 100))\n",
    "background_2d = np.column_stack([background_x.ravel(), background_y.ravel()])\n",
    "probabilities = model_with_softmax(torch.tensor(background_2d, dtype=torch.float32)).detach().numpy()\n",
    "\n",
    "# draw contour lines where the probabilities cross the 50% threshold\n",
    "ax.contour(background_x, background_y, probabilities[:, 0].reshape(background_x.shape), [0.5])\n",
    "ax.contour(background_x, background_y, probabilities[:, 1].reshape(background_x.shape), [0.5])\n",
    "ax.contour(background_x, background_y, probabilities[:, 2].reshape(background_x.shape), [0.5])\n",
    "\n",
    "ax.set_xlabel(\"Bill length (mm)\")\n",
    "ax.set_ylabel(\"Bill depth (mm)\")\n",
    "ax.axis([29, 62, 12, 23])\n",
    "\n",
    "ax.legend([\"Adelie\", \"Gentoo\", \"Chinstrap\"])\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe49568-91ca-4cbe-9706-9d9b1710d54f",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a6b135-7127-48ef-8b71-082761abcbb9",
   "metadata": {},
   "source": [
    "## Optimizers: learning rate, epochs, mini-batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f57f6c1-aee8-4912-befb-624ad9f08bb6",
   "metadata": {},
   "source": [
    "Since neural networks rely so heavily on function minimization, this topic has been heavily researched.\n",
    "\n",
    "PyTorch has a large set of [optimization algorithms](https://pytorch.org/docs/stable/optim.html) and recommendations. [So does TensorFlow.](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)\n",
    "\n",
    "Generally, [Adam or AdamW is the best choice](https://www.fast.ai/posts/2018-07-02-adam-weight-decay.html), but you need to know how to parameterize it. The most important considerations are below.\n",
    "\n",
    "1. The learning rate (`lr`): how large of a step size should the optimizer take? If it's too large, it can overshoot the minimum, and if it's too small, it will need more steps (\"epochs\") to reach the minimum.\n",
    "2. Number of epochs (usually a `for` loop you write, but some algorithms take the body of the loop as a `closure` function it choose when to call it).\n",
    "3. Number of mini-batches per epoch (also a `for` loop). I'll discuss this later.\n",
    "4. Some algorithms use first-order derivatives of the loss function (computed automatically by calling `loss.backward()`).\n",
    "5. Some algorithms have a configurable momentum (`betas`) that increase or decrease the step size. [This is a great explanation of the concept.](https://distill.pub/2017/momentum/)\n",
    "6. The initial parameter values matter, too. (Fixing a problem in initial values contributed to the current boom in deep learning.) This can be controlled with initializers from [nn.init](https://pytorch.org/docs/master/nn.init.html), but the default for a linear layer of $n$ inputs is uniform in\n",
    "\n",
    "$$ -\\frac{1}{\\sqrt{n}} < \\mbox{initial weight} < \\frac{1}{\\sqrt{n}} $$\n",
    "\n",
    "7. Flat sections of activation functions: if an input value to a sigmoid function is far from zero, the derivative is small and the optimizer won't know which direction to go. (\"Vanishing gradients\": also a problem whose solution contributed to the current boom in deep learning.)\n",
    "\n",
    "<table>\n",
    "    <tr style=\"background: white;\"><td style=\"text-align: center;\">Sigmoid (logistic)</td></tr>\n",
    "    <tr style=\"background: white;\"><td style=\"text-align: center;\"><img src=\"../img/Activation_logistic.svg\" width=\"200\"></td></tr>\n",
    "</table>\n",
    "\n",
    "The ReLU, Leaky ReLU, and Swish functions solve this in different ways. (ReLU is the most common.)\n",
    "\n",
    "<table>\n",
    "    <tr style=\"background: white;\"><td style=\"text-align: center;\">ReLU</td><td style=\"text-align: center;\">Leaky ReLU</td><td style=\"text-align: center;\">Swish</td></tr>\n",
    "    <tr style=\"background: white;\"><td style=\"text-align: center;\"><img src=\"../img/Activation_rectified_linear.svg\" width=\"200\"></td><td style=\"text-align: center;\"><img src=\"../img/Activation_prelu.svg\" width=\"200\"></td><td style=\"text-align: center;\"><img src=\"../img/Activation_swish.svg\" width=\"200\"></td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faf9fae-3465-4df5-be50-94a4d22944a0",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3f91af-226c-4a2f-b60f-e1510404caee",
   "metadata": {},
   "source": [
    "### 5-minute exercise: optimize a 2D function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b52b1d-4f37-4d08-8495-c5eac2aea54d",
   "metadata": {},
   "source": [
    "To get some intuition about the optimization algorithms, try to find the minimum of the [Beale function](https://www.sfu.ca/~ssurjano/beale.html).\n",
    "\n",
    "The true minimum of this function is at (3, 0.5) and has a value of 0.\n",
    "\n",
    "(Note: this is not something you'd normally do, since most models have thousands of parameters and can't be visualized. The purpose of this exercise is to help you get a sense of what the parameters mean.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d2cc1c-a1a4-4eaf-81c1-bf7fe445c520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beale(x, y):\n",
    "    return (1.5 - x + x*y)**2 + (2.25 - x + x*y**2)**2 + (2.625 - x + x*y**3)**2\n",
    "\n",
    "# a PyTorch \"model\" that just consists of two optimizable parameters\n",
    "class Position2D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.xy = nn.Parameter(torch.tensor([-1, 2], dtype=torch.float32, requires_grad=True))\n",
    "    def forward(self):\n",
    "        return self.xy[0], self.xy[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c1f686-e75a-454c-a0bd-76438ceff4c5",
   "metadata": {},
   "source": [
    "Define a optimizer than can find the true minimum of (3, 0.5), replacing the naive one in this cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b98e184-84af-4fb3-837c-62bd274e66bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Position2D()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cf2276-0156-4995-859d-0e5f0e7819ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_start = list(model.parameters())[0].detach().numpy().tolist()\n",
    "\n",
    "path = [path_start]\n",
    "for epoch in range(1000):\n",
    "\n",
    "    # start an optimization step in the usual way\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # this model has no arguments, and its loss function is beale (unusual!)\n",
    "    loss = beale(*model())\n",
    "\n",
    "    # finish an optimization step in the usual way\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # keep track of the path, to plot it\n",
    "    path.append(list(model.parameters())[0].detach().numpy().tolist())\n",
    "\n",
    "path_np = np.array(path)\n",
    "\n",
    "print(\"Final position, should be (3, 0.5):\")\n",
    "print(list(model.parameters())[0])\n",
    "print()\n",
    "print(\"Final loss, should be 0:\")\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781a39d2-c14d-4367-b80c-4bd86f4dafd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "background_x, background_y = np.meshgrid(np.linspace(-4.5, 4.5, 1000), np.linspace(-4.5, 4.5, 1000))\n",
    "contours = ax.contour(background_x, background_y, beale(background_x, background_y), [1, 3, 5, 10, 30, 50, 100, 300, 500, 1000], norm=\"log\")\n",
    "ax.clabel(contours, contours.levels)\n",
    "ax.axis([-4.5, 4.5, -4.5, 4.5])\n",
    "\n",
    "ax.scatter([3], [0.5], marker=\"*\", s=1000, c=\"red\")\n",
    "ax.scatter([path_start[0]], [path_start[1]], marker=\"o\", s=300, c=\"red\")\n",
    "ax.plot(path_np[:, 0], path_np[:, 1], c=\"red\")\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfb304b-9aa6-4961-8a6e-3c3dd9649cb2",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b141dc1d-c6e9-45fc-af42-7eb57a669c6b",
   "metadata": {},
   "source": [
    "### Epochs and mini-batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8454383f-d06f-4a5f-b9aa-1daeb3147b39",
   "metadata": {},
   "source": [
    "These aspects of optimization are under your control because you write the loops.\n",
    "\n",
    "We've seen \"epochs\" before: it is the main optimizer loop.\n",
    "\n",
    "Whereas Minuit iterates until it converges, most neural networks are iterated a large, fixed number of times. But since we have control, we can implement that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70da223-f59d-474b-ad96-8ed29ac79b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Position2D()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e8a10a-c021-4f33-92cc-c17df4f1576c",
   "metadata": {},
   "source": [
    "Keep going until no input parameter changes by no more than 0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303e8db4-33d6-4389-b0cf-2773457d43ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_parameters = None\n",
    "this_parameters = None\n",
    "\n",
    "epoch = 0\n",
    "while last_parameters is None or np.max(abs(this_parameters - last_parameters)) > 0.001:\n",
    "    epoch += 1\n",
    "    last_parameters = this_parameters\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss = beale(*model())\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    this_parameters = list(model.parameters())[0].detach().numpy().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbde1d11-9ed8-4ed2-94a8-9092a281041c",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4189ce96-d39d-4a7f-ad9e-a2656a66e17f",
   "metadata": {},
   "source": [
    "(We could also have stopped when the change in `loss` is smaller than some threshold.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666e7c55-5bab-4ebf-ae15-a2ba8921d104",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196f0ea6-df1d-4c55-bb66-d457b747c493",
   "metadata": {},
   "source": [
    "Mini-batches are loops within the loop over epochs:\n",
    "\n",
    "```python\n",
    "for epoch in range(NUMBER_OF_EPOCHS):\n",
    "\n",
    "    for batch_start in range(0, len(dataset), BATCH_SIZE):\n",
    "\n",
    "        data_subset = dataset[batch_start : batch_start + BATCH_SIZE]\n",
    "        optimization_step(data_subset)\n",
    "```\n",
    "\n",
    "<br><br>\n",
    "\n",
    "Why would you do that?\n",
    "\n",
    "Here's an illustration, using a linear fit for Boston housing prices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8665b7e5-1a91-411e-8599-8590a798db52",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = torch.tensor(boston_prices_df.drop(columns=\"MEDV\").values).float()\n",
    "targets = torch.tensor(boston_prices_df[\"MEDV\"]).float()[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede6cb11-8a6f-4f5a-99c6-c1ce8de74641",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3be45fa-97e1-426f-9fde-7837fa8847e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(12345)\n",
    "\n",
    "model = nn.Linear(features.shape[1], 1)\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "loss_vs_epoch = []\n",
    "for epoch in range(300):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(features)\n",
    "    loss = loss_function(predictions, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_vs_epoch.append(loss.item() * len(features))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(range(len(loss_vs_epoch)), loss_vs_epoch)\n",
    "\n",
    "ax.set_xlabel(\"epoch number\")\n",
    "ax.set_ylabel(\"loss × number of features\")\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec06c2e6-887b-4a43-8c2a-9feecbc4b32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(12345)\n",
    "\n",
    "model = nn.Linear(features.shape[1], 1)\n",
    "\n",
    "loss_function_batched = nn.MSELoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "loss_vs_epoch_batched = []\n",
    "for epoch in range(300):\n",
    "    total_loss = 0\n",
    "\n",
    "    BATCH_SIZE = 100\n",
    "    for batch_start in range(0, len(features), BATCH_SIZE):\n",
    "        batch_stop = batch_start + BATCH_SIZE\n",
    "\n",
    "        features_subset = features[batch_start:batch_stop]\n",
    "        targets_subset = targets[batch_start:batch_stop]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(features_subset)\n",
    "        loss = loss_function(predictions, targets_subset)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * len(features_subset)\n",
    "\n",
    "    loss_vs_epoch_batched.append(total_loss)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(range(len(loss_vs_epoch)), loss_vs_epoch)\n",
    "ax.plot(range(len(loss_vs_epoch_batched)), loss_vs_epoch_batched)\n",
    "\n",
    "ax.set_xlabel(\"epoch number\")\n",
    "ax.set_ylabel(\"loss × number of features\")\n",
    "ax.legend([\"one big batch\", f\"mini-batches of size {BATCH_SIZE}\"])\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a727833-b13f-49b1-8391-8f111a073c9f",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08aecb7a-53ea-46d1-bf55-5efd273f543d",
   "metadata": {},
   "source": [
    "Convergence is faster, but why?\n",
    "\n",
    "Mini-batches are a compromise between two extremes:\n",
    "\n",
    "* Batch gradient descent (e.g. Minuit), which uses the whole dataset in each optimizer update. It can get caught in saddle points and long, narrow valleys.\n",
    "* Stochastic gradient descent, which updates the model with each input data point. It can be noisy.\n",
    "\n",
    "<img src=\"../img/why-mini-batches.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8300b8-8b78-4e5e-b890-5189d34d4099",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0a0996-528f-4e48-9b41-2734d9eb14cd",
   "metadata": {},
   "source": [
    "Mini-batches don't need to be contiguous in the input data, and it's sometimes better if they're not.\n",
    "\n",
    "What would happen if this dataset was mini-batched?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11dd088-6db3-47e7-b09c-07e6cc31334e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(range(len(stockmarket_data)), stockmarket_data)\n",
    "\n",
    "ax.set_xlabel(\"minutes after July 26, 2016\")\n",
    "ax.set_ylabel(\"NASDAQ value of NXPI\")\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd03c6ce-5b34-431a-9ee9-daefa42d3522",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c512e9a6-24eb-47f4-9ae8-705b3e617091",
   "metadata": {},
   "source": [
    "PyTorch has as [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html) to split a dataset into batches consistently, and optionally shuffle it to remove time-series correlations.\n",
    "\n",
    "The TensorFlow equivalent is [tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) (see the [batch](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch) and [shuffle](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle) methods)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc60ea9-9059-48c7-8c86-0bab2462e9c7",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5a9ff5-0fb5-45e3-9f63-0e9637364ac8",
   "metadata": {},
   "source": [
    "## Feature selection and the \"kernel trick\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365a4257-61e1-4f28-9dbb-338432f527f3",
   "metadata": {},
   "source": [
    "As you saw in the TensorFlow Playground, non-linear combinations of input features can be as good as or better than adding hidden layers.\n",
    "\n",
    "<img src=\"../img/playground-feature-selection.png\" width=\"200\">\n",
    "\n",
    "Adding these features allows a linear classifier to solve (some!) non-linear problems.\n",
    "\n",
    "This method was popularized for Support Vector Machines (SVMs), but it applies to any linear classifier, including a neural network layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8cfe13-4648-4e51-9703-1f4d0d58eae4",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f815ee-8df4-4f8b-9c31-5bdfed426f8e",
   "metadata": {},
   "source": [
    "Demonstrate with the \"circle\" problem from TensorFlow Playground:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82b0f55-aa30-4393-8ada-a28325286503",
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_r = abs(np.random.normal(0, 1, 1000))\n",
    "outer_r = abs(np.random.normal(3, 0.5, 1000))\n",
    "inner_phi = np.random.uniform(0, 2*np.pi, 1000)\n",
    "outer_phi = np.random.uniform(0, 2*np.pi, 1000)\n",
    "\n",
    "inner_x = inner_r * np.cos(inner_phi)\n",
    "inner_y = inner_r * np.sin(inner_phi)\n",
    "outer_x = outer_r * np.cos(outer_phi)\n",
    "outer_y = outer_r * np.sin(outer_phi)\n",
    "\n",
    "all_x = np.concatenate([inner_x, outer_x])\n",
    "all_y = np.concatenate([inner_y, outer_y])\n",
    "targets = np.concatenate([np.zeros(1000), np.ones(1000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8007dfd8-3961-474d-92b7-6effc501a715",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_2d = np.column_stack([all_x, all_y])\n",
    "features_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351de448-a760-489e-8544-9e3f8a87dca5",
   "metadata": {},
   "source": [
    "Linear classification can't find the decision boundary in 2 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980051ef-d9fc-4729-9b65-647c5a0da2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sklearn.linear_model.LogisticRegression(penalty=None)\n",
    "\n",
    "model.fit(features_2d, targets)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "ax.scatter(inner_x, inner_y, marker=\".\")\n",
    "ax.scatter(outer_x, outer_y, marker=\".\")\n",
    "\n",
    "background_x, background_y = np.meshgrid(np.linspace(-4, 4, 100), np.linspace(-4, 4, 100))\n",
    "background_2d = np.column_stack([background_x.ravel(), background_y.ravel()])\n",
    "probabilities = model.predict_proba(background_2d)\n",
    "\n",
    "ax.contour(background_x, background_y, probabilities[:, 0].reshape(background_x.shape), [0.5], linewidths=[4])\n",
    "\n",
    "ax.axis([-4, 4, -4, 4])\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc9caca-e452-46a1-a0f1-bd24c0817fca",
   "metadata": {},
   "source": [
    "But if the third input feature is $x^2 + y^2$,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89d04bb-48e5-4c15-b3d4-663ae1488c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_3d = np.column_stack([all_x, all_y, all_x**2 + all_y**2])\n",
    "features_3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75af6781-a1b4-42b0-8612-4d21eef904ad",
   "metadata": {},
   "source": [
    "Linear classification can find the decision boundary in these 3 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07b60a8-c75d-4ebc-bde5-e8274f60c60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sklearn.linear_model.LogisticRegression(penalty=None)\n",
    "\n",
    "model.fit(features_3d, targets)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "ax.scatter(inner_x, inner_y, marker=\".\")\n",
    "ax.scatter(outer_x, outer_y, marker=\".\")\n",
    "\n",
    "background_x, background_y = np.meshgrid(np.linspace(-4, 4, 100), np.linspace(-4, 4, 100))\n",
    "background_x2y2 = background_x**2 + background_y**2\n",
    "background_3d = np.column_stack([background_x.ravel(), background_y.ravel(), background_x2y2.ravel()])\n",
    "probabilities = model.predict_proba(background_3d)\n",
    "\n",
    "ax.contour(background_x, background_y, probabilities[:, 0].reshape(background_x.shape), [0.5], linewidths=[4])\n",
    "\n",
    "ax.axis([-4, 4, -4, 4])\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b5d4bb-e0a4-4b96-89fd-06eadbf394c9",
   "metadata": {},
   "source": [
    "Because this third dimension separates the points in a way that can be sliced with a linear (planar) boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa14d678-3777-4dad-8f59-a5c914d0d3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "ax.set_box_aspect(None, zoom=1.5)\n",
    "\n",
    "ax.scatter(features_3d[targets == 0, 0], features_3d[targets == 0, 1], zs=features_3d[targets == 0, 2], marker=\".\")\n",
    "ax.scatter(features_3d[targets == 1, 0], features_3d[targets == 1, 1], zs=features_3d[targets == 1, 2], marker=\".\")\n",
    "\n",
    "background_x, background_y = np.meshgrid(np.linspace(-4, 4, 2), np.linspace(-4, 4, 2))\n",
    "ax.plot_surface(background_x, background_y, np.full_like(background_x, 3.63636364), facecolors=[\"green\"], alpha=0.1)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f5b0c0-0225-4cd1-b345-0b116fcfa949",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3f22b6-91f6-4605-82c7-582c3c36ac00",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796c6a6e-e3d8-4e2c-91e7-b1ede13e1888",
   "metadata": {},
   "source": [
    "This is why Taylor and Fourier series are a kind of linear fitting: even a single feature $x$ can be used as an $n$-dimensional space of $x$, $x^2$, $x^3$, ... $x^n$ (or similarly for $\\sin x$, $\\cos x$, $\\sin 2x$, $\\cos 2x$, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b7a177-e6e0-4a2d-985d-11a8a520967b",
   "metadata": {},
   "outputs": [],
   "source": [
    "just_increasing_numbers = np.linspace(0, 1, len(stockmarket_data))[:, np.newaxis]\n",
    "just_increasing_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88afdc1-e3dc-4f04-9c40-2914e62caaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_features = np.hstack([just_increasing_numbers**i for i in range(1, 30)])\n",
    "polynomial_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74863920-7353-48bc-8d5f-cf5645a5e342",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sklearn.linear_model.LinearRegression()\n",
    "\n",
    "model.fit(polynomial_features, np.array(stockmarket_data))\n",
    "\n",
    "model_y = model.predict(polynomial_features)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(just_increasing_numbers, stockmarket_data, marker=\".\")\n",
    "ax.plot(just_increasing_numbers, model_y, color=\"orange\", linewidth=3)\n",
    "\n",
    "ax.set_xlabel(\"normalized time\")\n",
    "ax.set_ylabel(\"NASDAQ value of NXPI\")\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e98f31d-32e4-4261-8b81-da0da20aee0e",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ced689c-95c7-430f-9e06-8ce30a577e5b",
   "metadata": {},
   "source": [
    "\"Engineering\" features can be useful if you know what's relevant to the problem (such as the circle).\n",
    "\n",
    "If you don't, it's better to let hidden layers (adaptive basis functions) discover the features for you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbd2898-43bc-4568-9d01-39fff87f0c78",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19b00de-3e2b-4961-90cc-d100665f0e52",
   "metadata": {},
   "source": [
    "## Under & overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2430b59d-c0e5-474b-b04c-140b491b0ce6",
   "metadata": {},
   "source": [
    "What do you think of this fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7706c627-fa33-414d-a02f-e0b51800bb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = np.linspace(0, 1, 10)\n",
    "data_y = -1 + 2*data_x + np.random.normal(0, 0.3, 10)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(data_x, data_y, s=100)\n",
    "\n",
    "ax.set_ylim(-2, 2)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4a0c22-533d-4adc-89d6-4199821dba54",
   "metadata": {},
   "outputs": [],
   "source": [
    "POLYNOMIAL_DEGREE = 10\n",
    "\n",
    "model = sklearn.linear_model.LinearRegression()\n",
    "model.fit(np.hstack([data_x[:, np.newaxis]**i for i in range(1, POLYNOMIAL_DEGREE)]), data_y[:, np.newaxis])\n",
    "\n",
    "model_x = np.linspace(0, 1, 1000)\n",
    "model_y = model.predict(np.hstack([model_x[:, np.newaxis]**i for i in range(1, POLYNOMIAL_DEGREE)]))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(data_x, data_y, s=100)\n",
    "ax.plot(model_x, model_y, color=\"orange\", linewidth=3)\n",
    "\n",
    "ax.set_ylim(-2, 2)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3554d2-3aed-46c4-8e02-49cbee0e7415",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3651be2b-b274-40f7-87b8-352a3ceb155c",
   "metadata": {},
   "source": [
    "Sometimes a model fits data too poorly, and sometimes it fits too well.\n",
    "\n",
    "Underfitting is bad just because it's inaccurate, but overfitting is more insidious because it confidently makes claims that aren't true.\n",
    "\n",
    "The data points have an underlying relationship (in the above case, it's linear: `-1 + 2*data_x`) and noise that is particular to the training dataset, which would not be the same in other data samples. The overfitted polynomial is wrong because it is sensitive to that particular noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d821db71-30da-4bf0-9edb-c247cc1a0765",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774211dd-bb3e-4683-a7c8-9a6f5ed17b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "adelie_df = penguins_df[penguins_df[\"species\"] == \"Adelie\"][[\"bill_length_mm\", \"bill_depth_mm\"]].dropna()\n",
    "gentoo_df = penguins_df[penguins_df[\"species\"] == \"Gentoo\"][[\"bill_length_mm\", \"bill_depth_mm\"]].dropna()\n",
    "chinstrap_df = penguins_df[penguins_df[\"species\"] == \"Chinstrap\"][[\"bill_length_mm\", \"bill_depth_mm\"]].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf8c527-ddb5-4f27-9439-c53e565bea04",
   "metadata": {},
   "outputs": [],
   "source": [
    "adelie_df[\"is_adelie\"] = 1\n",
    "gentoo_df[\"is_gentoo\"] = 1\n",
    "chinstrap_df[\"is_chinstrap\"] = 1\n",
    "\n",
    "onehot_df = pd.concat([adelie_df, gentoo_df, chinstrap_df]).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e13bd21-be3b-4482-8248-1cdfb7d9c392",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_np = onehot_df[[\"bill_length_mm\", \"bill_depth_mm\"]].values\n",
    "targets_np = onehot_df[[\"is_adelie\", \"is_gentoo\", \"is_chinstrap\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3eff1d-bd41-4c64-beb2-ea2efa491f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = torch.tensor(features_np, dtype=torch.float32)\n",
    "targets = torch.tensor(targets_np, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dfe805-a022-4270-a9cd-975a5ad42abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeInput(nn.Module):\n",
    "    def __init__(self, mean1, std1, mean2, std2):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"mean\", torch.tensor([mean1, mean2], dtype=torch.float32))\n",
    "        self.register_buffer(\"std\", torch.tensor([std1, std2], dtype=torch.float32))\n",
    "    def forward(self, x):\n",
    "        return (x - self.mean) / self.std\n",
    "\n",
    "normalize_input = NormalizeInput(\n",
    "    features_np[:, 0].mean(), features_np[:, 0].std(),\n",
    "    features_np[:, 1].mean(), features_np[:, 1].std(),\n",
    ")\n",
    "# normalize_input.forward(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8ef84e-1a9a-4aac-a773-5d1611a5e388",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_without_softmax = nn.Sequential(\n",
    "    normalize_input,\n",
    "    nn.Linear(2, 3),\n",
    ")\n",
    "# model_without_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da7bc4e-467b-4fd7-81cc-afa7d21a9dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_softmax = nn.Sequential(\n",
    "    model_without_softmax,\n",
    "    nn.Softmax(dim=1),\n",
    ")\n",
    "# model_with_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b5077c-8aec-4cc3-8d51-743428bb7a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(model_without_softmax.parameters(), lr=0.03)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    predictions = model_without_softmax(features)\n",
    "\n",
    "    loss = loss_function(predictions, targets)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89fd65b-eb11-4364-8888-6f14b6d0e79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# draw three scatter plots, each a different color\n",
    "ax.scatter(features_np[targets_np[:, 0] == 1, 0], features_np[targets_np[:, 0] == 1, 1])\n",
    "ax.scatter(features_np[targets_np[:, 1] == 1, 0], features_np[targets_np[:, 1] == 1, 1])\n",
    "ax.scatter(features_np[targets_np[:, 2] == 1, 0], features_np[targets_np[:, 2] == 1, 1])\n",
    "\n",
    "# compute the three probabilities for every 2D point in the background\n",
    "background_x, background_y = np.meshgrid(np.linspace(29, 62, 100), np.linspace(12, 23, 100))\n",
    "background_2d = np.column_stack([background_x.ravel(), background_y.ravel()])\n",
    "probabilities = model_with_softmax(torch.tensor(background_2d, dtype=torch.float32)).detach().numpy()\n",
    "\n",
    "# draw contour lines where the probabilities cross the 50% threshold\n",
    "ax.contour(background_x, background_y, probabilities[:, 0].reshape(background_x.shape), [0.5])\n",
    "ax.contour(background_x, background_y, probabilities[:, 1].reshape(background_x.shape), [0.5])\n",
    "ax.contour(background_x, background_y, probabilities[:, 2].reshape(background_x.shape), [0.5])\n",
    "\n",
    "ax.set_xlabel(\"Bill length (mm)\")\n",
    "ax.set_ylabel(\"Bill depth (mm)\")\n",
    "ax.axis([29, 62, 12, 23])\n",
    "\n",
    "ax.legend([\"Adelie\", \"Gentoo\", \"Chinstrap\"])\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19055ef-d791-4308-99ff-4856006e6c4d",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f79eda-1416-4899-b607-cbf09b5509ea",
   "metadata": {},
   "source": [
    "## Regularization: L1, L2, dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de00e3ef-945f-4130-b70f-e1d363db635b",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cea7f5c-f2e6-42b8-bf62-241b110b5830",
   "metadata": {},
   "source": [
    "## Parameters versus hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faa3d01-536f-4bdc-8912-ed90106a8728",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806851fb-200c-478f-895c-de0bfbc47d6b",
   "metadata": {},
   "source": [
    "## Partitioning data into train-test-validate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415c3f35-9607-4569-96b4-3a595b434f5b",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f116d96-c26a-48d7-8c6e-2c0f593fc445",
   "metadata": {},
   "source": [
    "## Goodness of fit metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
