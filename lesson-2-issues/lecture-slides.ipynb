{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16f240ed-4887-4955-b015-51be367377e1",
   "metadata": {},
   "source": [
    "# Issues in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb77b630-053f-4b00-a60f-be62554cac66",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b4d7d8-3a44-461f-be35-54dcaabc0eb5",
   "metadata": {},
   "source": [
    "## Which library to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7d467d-d23b-4bf5-a830-d61633323988",
   "metadata": {},
   "source": [
    "**Many (canned) algorithms:**\n",
    "\n",
    "<table>\n",
    "    <tr style=\"background: white;\">\n",
    "        <td><img src=\"https://upload.wikimedia.org/wikipedia/commons/0/05/Scikit_learn_logo_small.svg\" style=\"height: 90px;\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "* Scikit-Learn's algorithms have a common interface, which makes it easy to learn a new one, but they're not flexible (only configurable by a long list of function arguments).\n",
    "\n",
    "<br><br>\n",
    "\n",
    "**Neural networks:**\n",
    "\n",
    "<table>\n",
    "    <tr style=\"background: white;\">\n",
    "        <td style=\"text-align: center; padding-right: 15px;\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/a/ab/TensorFlow_logo.svg\" style=\"height: 140px;\"></td>\n",
    "        <td style=\"text-align: center; padding-left: 15px;\"><img src=\"https://keras.io/img/logo.png\" style=\"height: 60px;\"></td>\n",
    "    </tr> <tr style=\"background: white;\">\n",
    "        <td style=\"text-align: center; padding-right: 15px;\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/c/c6/PyTorch_logo_black.svg\" style=\"height: 45px;\"></td>\n",
    "        <td style=\"text-align: center; padding-left: 15px;\"><img src=\"https://raw.githubusercontent.com/valohai/ml-logos/master/mxnet.svg\" style=\"height: 60px;\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "* TensorFlow and Keras merged, with Keras becoming the high-level interface to TensorFlow.\n",
    "* TensorFlow tends to be associated more with industry and production environments; PyTorch tends to be associated more with academic research.\n",
    "  * In PyTorch, the training loop (feeding the algorithm batches of data to fit) is just a Python for loop, which makes it easy to develop and debug.\n",
    "  * In TensorFlow 2.0 (2019) and later, training can be done this way as well.\n",
    "  * Both can be hard to install if you're trying to use a GPU (hard to match your CUDA library version).\n",
    "* MXNet is much less popular than TensorFlow and PyTorch, and development ended recently (2023).\n",
    "\n",
    "<br><br>\n",
    "\n",
    "**Neural network-capable array library:**\n",
    "\n",
    "<table>\n",
    "    <tr style=\"background: white;\">\n",
    "        <td><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/86/Google_JAX_logo.svg\" style=\"height: 75px;\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "* JAX is a NumPy replacement with support for GPU, autodiff, and JIT-compilation, which makes it possible to build new algorithms from scratch.\n",
    "  * Same installation troubles with GPUs/CUDA.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "**Boosted decision trees:**\n",
    "\n",
    "<table>\n",
    "    <tr style=\"background: white;\">\n",
    "        <td style=\"text-align: center; padding-right: 30px; padding-bottom: 25px;\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/6/69/XGBoost_logo.png\" style=\"height: 60px;\"></td>\n",
    "        <td style=\"text-align: center;\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/d/d9/LightGBM_logo_black_text.svg\" style=\"height: 45px;\"></td>\n",
    "        <td style=\"text-align: center; padding-left: 30px;\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/c/cc/CatBoostLogo.png\" style=\"height: 70px; vertical-align: middle; padding-bottom: 15px;\"> <span style=\"font-size: 18pt;\">CatBoost</span></td>\n",
    "</table>\n",
    "\n",
    "* Boosted decision trees are still relevant for problems with well-chosen input features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e9e185-30de-4145-8841-a2ce384e7e22",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef2bcce-62bd-4a57-94d9-5ce113362d3a",
   "metadata": {},
   "source": [
    "### What does everyone else use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08ffa03-3e41-4055-898a-b17fa501ad02",
   "metadata": {},
   "source": [
    "Using techniques described [here](https://indico.jlab.org/event/459/contributions/11547/) and [here](https://github.com/jpivarski-talks/2023-05-09-chep23-analysis-of-physicists), this is the number of times each ML library is imported, semiannually, in code written by CMS physicists:\n",
    "\n",
    "<img src=\"../img/github-ml-package-cmsswseed.svg\" width=\"900\">\n",
    "\n",
    "* Scikit-Learn is oldest and still widely used\n",
    "* TensorFlow is dominant (and used to be imported with Keras, but not so much anymore)\n",
    "* PyTorch is increasingly significant\n",
    "* XGBoost is also common\n",
    "* JAX and the other libraries are not widely used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97a2aa5-a214-43a2-b7b1-8c8151e329a2",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57375b34-290c-4ecb-8667-3b4bdb0412dc",
   "metadata": {},
   "source": [
    "Using Google Trends (world search volume, not just physicists):\n",
    "\n",
    "<img src=\"../img/google-ml-package.svg\" width=\"900\">\n",
    "\n",
    "* PyTorch saw a burst of interest since 2023, but it's because of LLMs:\n",
    "\n",
    "<img src=\"../img/google-ml-llm-package.svg\" width=\"900\">\n",
    "\n",
    "* That's not relevant for HEP, though it would be interesting to extend my analysis of CMS physicsts one more year."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c797f4-139e-4e53-b1bd-7382e6c97d9f",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f2c69e-e7de-4d1f-9b36-501129e3e426",
   "metadata": {},
   "source": [
    "### What will this mini-course use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee7118a-c98f-4a8b-b8b1-e4e0a06bed62",
   "metadata": {},
   "source": [
    "Scikit-Learn for linear fits and simple neural networks.\n",
    "\n",
    "PyTorch for neural network architectures, because it's easier to illustrate the parts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd1b4c0-5f39-42a2-98e3-1e3be673007a",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
