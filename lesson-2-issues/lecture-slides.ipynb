{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16f240ed-4887-4955-b015-51be367377e1",
   "metadata": {},
   "source": [
    "# Issues in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb77b630-053f-4b00-a60f-be62554cac66",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b4d7d8-3a44-461f-be35-54dcaabc0eb5",
   "metadata": {},
   "source": [
    "## Which library to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7d467d-d23b-4bf5-a830-d61633323988",
   "metadata": {},
   "source": [
    "**Many (canned) algorithms:**\n",
    "\n",
    "<table>\n",
    "    <tr style=\"background: white;\">\n",
    "        <td><img src=\"https://upload.wikimedia.org/wikipedia/commons/0/05/Scikit_learn_logo_small.svg\" style=\"height: 90px;\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "* Scikit-Learn's algorithms have a common interface, which makes it easy to learn a new one, but they're not flexible (only configurable by a long list of function arguments).\n",
    "\n",
    "<br><br>\n",
    "\n",
    "**Neural networks:**\n",
    "\n",
    "<table>\n",
    "    <tr style=\"background: white;\">\n",
    "        <td style=\"text-align: center; padding-right: 15px;\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/a/ab/TensorFlow_logo.svg\" style=\"height: 140px;\"></td>\n",
    "        <td style=\"text-align: center; padding-left: 15px;\"><img src=\"https://keras.io/img/logo.png\" style=\"height: 60px;\"></td>\n",
    "    </tr> <tr style=\"background: white;\">\n",
    "        <td style=\"text-align: center; padding-right: 15px;\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/c/c6/PyTorch_logo_black.svg\" style=\"height: 45px;\"></td>\n",
    "        <td style=\"text-align: center; padding-left: 15px;\"><img src=\"https://raw.githubusercontent.com/valohai/ml-logos/master/mxnet.svg\" style=\"height: 60px;\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "* TensorFlow and Keras merged, with Keras becoming the high-level interface to TensorFlow.\n",
    "* TensorFlow tends to be associated more with industry and production environments; PyTorch tends to be associated more with academic research.\n",
    "  * In PyTorch, the training loop (feeding the algorithm batches of data to fit) is just a Python for loop, which makes it easy to develop and debug.\n",
    "  * In TensorFlow 2.0 (2019) and later, training can be done this way as well.\n",
    "  * Both can be hard to install if you're trying to use a GPU (hard to match your CUDA library version).\n",
    "* MXNet is much less popular than TensorFlow and PyTorch, and development ended recently (2023).\n",
    "\n",
    "<br><br>\n",
    "\n",
    "**Neural network-capable array library:**\n",
    "\n",
    "<table>\n",
    "    <tr style=\"background: white;\">\n",
    "        <td><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/86/Google_JAX_logo.svg\" style=\"height: 75px;\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "* JAX is a NumPy replacement with support for GPU, autodiff, and JIT-compilation, which makes it possible to build new algorithms from scratch.\n",
    "  * Same installation troubles with GPUs/CUDA.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "**Boosted decision trees:**\n",
    "\n",
    "<table>\n",
    "    <tr style=\"background: white;\">\n",
    "        <td style=\"text-align: center; padding-right: 30px; padding-bottom: 25px;\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/6/69/XGBoost_logo.png\" style=\"height: 60px;\"></td>\n",
    "        <td style=\"text-align: center;\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/d/d9/LightGBM_logo_black_text.svg\" style=\"height: 45px;\"></td>\n",
    "        <td style=\"text-align: center; padding-left: 30px;\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/c/cc/CatBoostLogo.png\" style=\"height: 70px; vertical-align: middle; padding-bottom: 15px;\"> <span style=\"font-size: 18pt;\">CatBoost</span></td>\n",
    "</table>\n",
    "\n",
    "* Boosted decision trees are still relevant for problems with well-chosen input features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e9e185-30de-4145-8841-a2ce384e7e22",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef2bcce-62bd-4a57-94d9-5ce113362d3a",
   "metadata": {},
   "source": [
    "### What does everyone else use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08ffa03-3e41-4055-898a-b17fa501ad02",
   "metadata": {},
   "source": [
    "Using techniques described [here](https://indico.jlab.org/event/459/contributions/11547/) and [here](https://github.com/jpivarski-talks/2023-05-09-chep23-analysis-of-physicists), this is the number of times each ML library is imported, semiannually, in code written by CMS physicists:\n",
    "\n",
    "<img src=\"../img/github-ml-package-cmsswseed.svg\" width=\"900\">\n",
    "\n",
    "* Scikit-Learn is oldest and still widely used\n",
    "* TensorFlow is dominant (and used to be imported with Keras, but not so much anymore)\n",
    "* PyTorch is increasingly significant\n",
    "* XGBoost is also common\n",
    "* JAX and the other libraries are not widely used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97a2aa5-a214-43a2-b7b1-8c8151e329a2",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57375b34-290c-4ecb-8667-3b4bdb0412dc",
   "metadata": {},
   "source": [
    "Using Google Trends (world search volume, not just physicists):\n",
    "\n",
    "<img src=\"../img/google-ml-package.svg\" width=\"900\">\n",
    "\n",
    "* PyTorch saw a burst of interest since 2023, but it's because of LLMs:\n",
    "\n",
    "<img src=\"../img/google-ml-llm-package.svg\" width=\"900\">\n",
    "\n",
    "* That's not relevant for HEP, though it would be interesting to extend my analysis of CMS physicsts one more year."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c797f4-139e-4e53-b1bd-7382e6c97d9f",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f2c69e-e7de-4d1f-9b36-501129e3e426",
   "metadata": {},
   "source": [
    "### What will this mini-course use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee7118a-c98f-4a8b-b8b1-e4e0a06bed62",
   "metadata": {},
   "source": [
    "Scikit-Learn for linear fits and simple neural networks.\n",
    "\n",
    "PyTorch for neural network architectures, because it's easier to illustrate the parts.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "Most of the examples in _this lesson_ are linear fits because the issues that I'll be discussing are general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339d0638-ac03-4c29-829c-6eb1c97607a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn.linear_model\n",
    "import torch\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256c3827-68b2-4046-9b58-4afcb5b3cf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins_df = pd.read_csv(\"../data/penguins.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd1b4c0-5f39-42a2-98e3-1e3be673007a",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cae10c1-b2ce-44c3-a7db-a924c737d3bc",
   "metadata": {},
   "source": [
    "## Regression versus classification, loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af8c041-47ae-4192-9aec-748654150a97",
   "metadata": {},
   "source": [
    "When we think of fitting, we usually think of regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6248a9b-1719-42ae-bcb1-b9d57a8a1280",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sklearn.linear_model.LinearRegression()\n",
    "model.fit(penguins_df.dropna()[\"flipper_length_mm\"].values[:, np.newaxis], penguins_df.dropna()[\"body_mass_g\"])\n",
    "\n",
    "model_x = np.linspace(170, 240, 2)\n",
    "model_y = model.predict(model_x[:, np.newaxis])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "penguins_df.plot.scatter(\"flipper_length_mm\", \"body_mass_g\", ax=ax)\n",
    "ax.plot(model_x, model_y, color=\"orange\", linewidth=3)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898aea1b-e80f-4bcf-8ca1-9387f83c737d",
   "metadata": {},
   "source": [
    "Given $x$ values drawn from a vector space, the model responds with the most likely $y$ values, which also come from a vector space (that is, they're real-valued, maybe more than one of them)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cad44c-1f2d-4980-93e3-f8cb51ddd27e",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cff40b3-7e8e-4dad-9427-87e633dc7b73",
   "metadata": {},
   "source": [
    "Sometimes, though, you're interested in _categorical_ data, like the colors of the dots in the TensorFlow Playground exercise.\n",
    "\n",
    "In general, there are [four levels of measurement](https://en.wikipedia.org/wiki/Level_of_measurement),\n",
    "\n",
    "| Level | Math | Description | Physics example |\n",
    "|:--|:--:|:--|:--|\n",
    "| Nominal category | =, ≠ | categories without order | jet classification, data versus Monte Carlo |\n",
    "| Ordinal category | >, < | categories that have an order | barrel region, overlap region, endcap region |\n",
    "| Interval number | +, ‒ | doesn't have an origin | energy, voltage, position, momentum |\n",
    "| Ratio number | ×, / | has an origin | absolute temperature, mass, opening angle |\n",
    "\n",
    "But generally, we only need to be concerned about categorical versus numerical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9f021c-3ecb-4e17-b07e-e23cf6b6f6bf",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0942e3e9-c076-46cd-b1c5-2714fe1abe54",
   "metadata": {},
   "source": [
    "Suppose we have data from two distinct categories that have different, but overlapping, distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe26747-2f2f-4de8-8a50-e245c27a4736",
   "metadata": {},
   "outputs": [],
   "source": [
    "adelie_df = penguins_df[penguins_df[\"species\"] == \"Adelie\"][[\"bill_length_mm\", \"bill_depth_mm\"]].dropna()\n",
    "gentoo_df = penguins_df[penguins_df[\"species\"] == \"Gentoo\"][[\"bill_length_mm\", \"bill_depth_mm\"]].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a337eb6-f11d-48e3-92a9-3214c12074ec",
   "metadata": {},
   "source": [
    "<img src=\"../img/culmen_depth.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333ad717-7e08-4c46-bc8d-74175b1d1f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "adelie_df[\"bill_depth_mm\"].plot.hist(alpha=0.5)\n",
    "gentoo_df[\"bill_depth_mm\"].plot.hist(alpha=0.5)\n",
    "\n",
    "ax.set_xlabel(\"Bill depth (mm)\")\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785e8c61-76fa-41f8-b171-1c4846b7d9d9",
   "metadata": {},
   "source": [
    "We can express the two categories as values 0 and 1 in a numeric dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1584db50-d4d4-402e-847b-6875a68f57fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "species = np.concatenate([np.zeros(len(adelie_df)), np.ones(len(gentoo_df))])\n",
    "bill_depth = np.concatenate([adelie_df[\"bill_depth_mm\"].values, gentoo_df[\"bill_depth_mm\"].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4533079-72ea-4e3f-8c94-0ab7490ac959",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(species, bill_depth, marker=\"x\")\n",
    "\n",
    "ax.set_xlabel(\"Species; 0 = Adelie, 1 = Gentoo\")\n",
    "ax.set_ylabel(\"Bill depth (mm)\")\n",
    "ax.set_xlim(-0.5, 1.5)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48edb93-fbe8-4f59-9248-e624e9f1c17e",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b7a36c-34ab-493d-8046-b7f9f38c7f76",
   "metadata": {},
   "source": [
    "If we are intending to use the categorical data as a _feature_, an input to the model, then we can fit it as-is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851d87ef-21c6-4985-b755-4d99ee027139",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sklearn.linear_model.LinearRegression()\n",
    "model.fit(species[:, np.newaxis], bill_depth)\n",
    "\n",
    "model_x = np.linspace(-0.5, 1.5, 2)\n",
    "model_y = model.predict(model_x[:, np.newaxis])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(species, bill_depth, marker=\"x\")\n",
    "ax.plot(model_x, model_y, color=\"orange\", linewidth=3)\n",
    "\n",
    "ax.set_xlabel(\"Species; 0 = Adelie, 1 = Gentoo\")\n",
    "ax.set_ylabel(\"Bill depth (mm)\")\n",
    "ax.set_xlim(-0.5, 1.5)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac7f932-a437-4558-bbfd-721ddb3791e6",
   "metadata": {},
   "source": [
    "The fit value at 0 is the average Adelie bill depth and the fit value at 1 is the average Gentoo bill depth.\n",
    "\n",
    "But if the model is supposed to _predict_ the categorical data, as an output of the model, then a linear fit is not meaningful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d85e53f-b9e2-43f9-b6a2-2c0f248efc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sklearn.linear_model.LinearRegression()\n",
    "model.fit(bill_depth[:, np.newaxis], species)\n",
    "\n",
    "model_x = np.linspace(12, 22, 2)\n",
    "model_y = model.predict(model_x[:, np.newaxis])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(bill_depth, species, marker=\"x\")\n",
    "ax.plot(model_x, model_y, color=\"orange\", linewidth=3)\n",
    "\n",
    "ax.set_xlabel(\"Bill depth (mm)\")\n",
    "ax.set_ylabel(\"Species; 0 = Adelie, 1 = Gentoo\")\n",
    "\n",
    "ax.text(18, 0.5, \"WRONG!!!\", color=\"red\", size=22)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9760f164-e93d-4b8d-b79d-7a7ddf4bb3df",
   "metadata": {},
   "source": [
    "Whether the model is predicting an allowed species value of 0 or 1, an intermediate value like 0.5, or an out-of-bounds value like 1.5, it's not expressing anything that means anything to us.\n",
    "\n",
    "Since a linear fit or neural network has to predict continuous numerical values, we can make them useful and interpret them as probabilities. In the training dataset, \"0 = Adelie, 1 = Gentoo\" can be interpreted as the probability of Gentoo.\n",
    "\n",
    "The sigmoid function from the previous lecture,\n",
    "\n",
    "$$ p(x) = \\frac{1}{1 + \\exp(x)} $$\n",
    "\n",
    "clamps the output value of the model between 0 and 1, and a loss function of\n",
    "\n",
    "<!--\n",
    "$$ \\mathcal{L}_k(x_i) = \\left\\{\\begin{array}{l l}\n",
    "-\\log (p_k(x_i)) & \\mbox{if species of } x_i \\mbox{ is } k \\\\\n",
    "-\\log (1 - p_k(x_i)) & \\mbox{if species of } x_i \\mbox{ is not } k \\\\\n",
    "\\end{array}\\right. $$\n",
    "-->\n",
    "\n",
    "$$ \\mbox{loss}(x_i, y_i) = -(y_i) \\log \\big[ p(x_i) \\big] - (1 - y_i) \\log \\big[ 1 - p(x_i) \\big] $$\n",
    "\n",
    "optimizes a linear model to predict probabilities $p(x_i)$ that match the given probabilities $y_i$.\n",
    "\n",
    "In Scikit-Learn, this is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67f3830-0178-4aca-a399-1def0fff44f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sklearn.linear_model.LogisticRegression(penalty=None)\n",
    "model.fit(bill_depth[:, np.newaxis], species)\n",
    "\n",
    "model_x = np.linspace(12, 22, 1000)\n",
    "model_y = model.predict_proba(model_x[:, np.newaxis])[:, 1]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(bill_depth, species, marker=\"x\")\n",
    "ax.plot(model_x, model_y, color=\"orange\", linewidth=3)\n",
    "\n",
    "ax.set_xlabel(\"Bill depth (mm)\")\n",
    "ax.set_ylabel(\"Probability that species is Gentoo\")\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7574df-6bc2-44f1-9ffe-242d3fd09493",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba4a084-fa50-4925-8ebb-8ddff9da9517",
   "metadata": {},
   "source": [
    "2 lines of code in Scikit-Learn becomes a class definition, 8 lines of initialization, and an explicit 5-line loop over training steps in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6719e9-27b3-4b5b-9e47-a7622780b54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this user-defined model step centers and scales the input\n",
    "# without this, the optimizer would require many more steps to find the minimum\n",
    "class NormalizeInput(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"mean\", torch.tensor([mean], dtype=torch.float32))\n",
    "        self.register_buffer(\"std\", torch.tensor([std], dtype=torch.float32))\n",
    "    def forward(self, x):\n",
    "        return (x - self.mean) / self.std\n",
    "\n",
    "normalize_input = NormalizeInput(bill_depth.mean(), bill_depth.std())\n",
    "\n",
    "model = nn.Sequential(      # define a 3-step model\n",
    "    normalize_input,        # step 1: center/scale the input\n",
    "    nn.Linear(1, 1),        # step 2: linear transformation (1D → 1D)\n",
    "    nn.Sigmoid(),           # step 3: pass output through a sigmoid\n",
    ")\n",
    "\n",
    "# convert the data into PyTorch Tensors, which are differentiable and can live on a GPU\n",
    "features = torch.tensor(bill_depth[:, np.newaxis], dtype=torch.float32)\n",
    "targets = torch.tensor(species[:, np.newaxis], dtype=torch.float32)\n",
    "\n",
    "# use Binary Cross Entropy as a loss function\n",
    "loss_function = nn.BCELoss()\n",
    "\n",
    "# use Adam as an optimizer with a (high) learning rate of 0.03\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.03)\n",
    "\n",
    "# iterate over the same data 1000 times (epochs)\n",
    "for epoch in range(1000):\n",
    "    # tell the optimizer to begin an optimization step\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # give the model the input features and ask it to compute its predictions\n",
    "    predictions = model(features)\n",
    "\n",
    "    # compute the loss between these predictions and the intended targets\n",
    "    loss = loss_function(predictions, targets)\n",
    "\n",
    "    # tell the loss function and optimizer to end an optimization step\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49ae6e5-de83-461f-9f94-210243d95ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_x = np.linspace(12, 22, 1000)\n",
    "model_y = model(torch.tensor(model_x[:, np.newaxis], dtype=torch.float32)).detach().numpy()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(bill_depth, species, marker=\"x\")\n",
    "ax.plot(model_x, model_y, color=\"orange\", linewidth=3)\n",
    "\n",
    "ax.set_xlabel(\"Bill depth (mm)\")\n",
    "ax.set_ylabel(\"Probability that species is Gentoo\")\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c37c4d5-e617-4928-ab9f-b8da313bd294",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29b1fe5-ebda-4765-a4b4-c40387f59fa6",
   "metadata": {},
   "source": [
    "PyTorch makes all of the steps explicit, and that's (eventually) a good thing because we can change any step of the process.\n",
    "\n",
    "The steps that distinguish regression from classification are\n",
    "\n",
    "* including [nn.Sigmoid](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html) as the last step of the `model`\n",
    "* using [nn.BCELoss](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html) instead of [nn.MSELoss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html) as the loss function.\n",
    "\n",
    "The `NormalizeInput` class is an example of a user-defined model step to center and scale the input. Without it, the fit would be much slower. If I pre-processed the data manually, I'd have to remember to apply the same pre-processing on the training and prediction data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9c1779-d959-45dd-a9da-dc5457a37c9a",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec8f3cb-f7e6-4a20-8ac6-d6b168c29e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601d6db2-27ff-4644-91d5-a06d964851df",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function(torch.tensor([1.0]), torch.tensor([1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8765e084-7128-49bb-8679-4e0940308419",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function(torch.tensor([2.0]), torch.tensor([1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41197b3d-901d-4db2-813e-1eacfae6624f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function(torch.tensor([3.0]), torch.tensor([1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f94ea21-8274-45bd-b8c4-7ff2ec8533fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function(torch.tensor([4.0]), torch.tensor([1.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154bbf47-7491-40f0-9c40-aea07f881131",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45632812-6e4a-4b89-8bde-12d968ffa8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39caaa2c-9f52-430d-88c3-067ad0675a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function(torch.tensor([0.0001]), torch.tensor([1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06108d64-1852-469a-9665-7e67aa1e669a",
   "metadata": {},
   "outputs": [],
   "source": [
    "-np.log(0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb8742f-c888-424f-92d4-2ed9dd950fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function(torch.tensor([0.001]), torch.tensor([1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a651c94d-9bd9-4b7a-bf95-e93a46aa52da",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function(torch.tensor([0.01]), torch.tensor([1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfcc05b-5c17-41ab-a9e2-eddf2466d0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function(torch.tensor([0.1]), torch.tensor([1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ac4a14-abc1-4866-bebc-8492b773353c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function(torch.tensor([0.5]), torch.tensor([1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df62d744-23f5-43d5-b26b-5ade5d7b8eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function(torch.tensor([1.0]), torch.tensor([1.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5995b13-2977-46a5-ba18-2194748dd711",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7522ef-f1c1-4c30-b7da-5366cf66f678",
   "metadata": {},
   "source": [
    "Now suppose that we have more than two classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a7b6d1-c99f-466c-85fe-fbb2d3ca0182",
   "metadata": {},
   "outputs": [],
   "source": [
    "adelie_df = penguins_df[penguins_df[\"species\"] == \"Adelie\"][[\"bill_length_mm\", \"bill_depth_mm\"]].dropna()\n",
    "gentoo_df = penguins_df[penguins_df[\"species\"] == \"Gentoo\"][[\"bill_length_mm\", \"bill_depth_mm\"]].dropna()\n",
    "chinstrap_df = penguins_df[penguins_df[\"species\"] == \"Chinstrap\"][[\"bill_length_mm\", \"bill_depth_mm\"]].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c59fd3d-f778-4f84-a763-189916fe24f3",
   "metadata": {},
   "source": [
    "Can we define a species variable that takes values 0, 1, and 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc381b5-2c34-40ac-9d18-b9ce2f3385dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "adelie_df[\"species\"] = 0\n",
    "gentoo_df[\"species\"] = 1\n",
    "chinstrap_df[\"species\"] = 2\n",
    "\n",
    "ordinal_df = pd.concat([adelie_df, gentoo_df, chinstrap_df])\n",
    "\n",
    "model = sklearn.linear_model.LinearRegression()\n",
    "model.fit(ordinal_df[\"species\"].values[:, np.newaxis], ordinal_df[\"bill_depth_mm\"].values)\n",
    "\n",
    "model_x = np.linspace(-0.5, 2.5, 2)\n",
    "model_y = model.predict(model_x[:, np.newaxis])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ordinal_df.plot.scatter(\"species\", \"bill_depth_mm\", ax=ax)\n",
    "ax.plot(model_x, model_y, color=\"orange\", linewidth=3)\n",
    "\n",
    "ax.set_xlabel(\"Species; 0 = Adelie, 1 = Gentoo, 2 = Chinstrap\")\n",
    "ax.set_ylabel(\"Bill depth (mm)\")\n",
    "\n",
    "ax.text(0.5, 20, \"WRONG!!!\", color=\"red\", size=22)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271fcd8b-5260-45b9-8aae-0edb49a06bc4",
   "metadata": {},
   "source": [
    "Unlike the case with only 2 values for species, the fit is now finding the _order_ and the _distances_ between the values 0, 1, and 2 to be meaningful: it's interpreting species as interval numbers, rather than nominal categories.\n",
    "\n",
    "(With more parameters in the fit, we can wash away that interpretation: for instance, fitting the three categories with a parabola that goes through the mean of each. Neural networks with enough parameters can lose knowledge of the order and distances between integer encodings, so it can be okay in those contexts.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a79754-4b27-4f16-8db4-17ce75c50203",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a46772-c32a-48d9-8dac-9acfcbb1057d",
   "metadata": {},
   "source": [
    "Instead, expand a categorical variable with $n$ categories into $n$ variables whose values are 0 and 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041794b0-7df4-4a1e-8b7e-1e49cd07fa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "adelie_df[\"is_adelie\"] = 1\n",
    "gentoo_df[\"is_gentoo\"] = 1\n",
    "chinstrap_df[\"is_chinstrap\"] = 1\n",
    "\n",
    "onehot_df = pd.concat([adelie_df, gentoo_df, chinstrap_df]).fillna(0)\n",
    "onehot_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dbc9c9-c5f3-45ae-b088-be3d47f2432a",
   "metadata": {},
   "source": [
    "For historical reasons, this is called a [one-hot encoding](https://en.wikipedia.org/wiki/One-hot).\n",
    "\n",
    "The three new features, `is_adelie`, `is_gentoo`, and `is_chinstrap`, can be interpreted as given probabilities, and therefore either 0 and 1. They're constrained to have `is_adelie + is_gentoo + is_chinstrap = 1`. In principle, we only need $n - 1$ variables, since one is determined by the others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011cd6db-71c2-4d3c-9b0a-d25955a7e937",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ba9b5a-325a-44dc-9ac7-d323bf9108c7",
   "metadata": {},
   "source": [
    "Here's a logistic regression in Scikit-Learn, using bill length and bill depth as 2 input features to predict 3 probabilities `is_adelie`, `is_gentoo`, `is_chinstrap`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bc39cf-06cd-4d01-9f65-d83e214cb555",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_np = onehot_df[[\"bill_length_mm\", \"bill_depth_mm\"]].values\n",
    "targets_np = onehot_df[[\"is_adelie\", \"is_gentoo\", \"is_chinstrap\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5fc864-14cf-4c97-94f2-baec33caf26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sklearn.linear_model.LogisticRegression(penalty=None)\n",
    "\n",
    "# Scikit-Learn wants the target index, rather than the probability values themselves, so np.argmax\n",
    "model.fit(features_np, np.argmax(targets_np, axis=1))\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e904dd98-70a6-4df3-a78a-df82afa68fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# draw three scatter plots, each a different color\n",
    "ax.scatter(features_np[targets_np[:, 0] == 1, 0], features_np[targets_np[:, 0] == 1, 1])\n",
    "ax.scatter(features_np[targets_np[:, 1] == 1, 0], features_np[targets_np[:, 1] == 1, 1])\n",
    "ax.scatter(features_np[targets_np[:, 2] == 1, 0], features_np[targets_np[:, 2] == 1, 1])\n",
    "\n",
    "# compute the three probabilities for every 2D point in the background\n",
    "background_x, background_y = np.meshgrid(np.linspace(29, 62, 100), np.linspace(12, 23, 100))\n",
    "background_2d = np.column_stack([background_x.ravel(), background_y.ravel()])\n",
    "probabilities = model.predict_proba(background_2d)\n",
    "\n",
    "# draw contour lines where the probabilities cross the 50% threshold\n",
    "ax.contour(background_x, background_y, probabilities[:, 0].reshape(background_x.shape), [0.5])\n",
    "ax.contour(background_x, background_y, probabilities[:, 1].reshape(background_x.shape), [0.5])\n",
    "ax.contour(background_x, background_y, probabilities[:, 2].reshape(background_x.shape), [0.5])\n",
    "\n",
    "ax.set_xlabel(\"Bill length (mm)\")\n",
    "ax.set_ylabel(\"Bill depth (mm)\")\n",
    "ax.axis([29, 62, 12, 23])\n",
    "\n",
    "ax.legend([\"Adelie\", \"Gentoo\", \"Chinstrap\"])\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadc95a8-70f7-4d60-8269-2a76917e16a6",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fce570-d6c8-4992-a54f-af69d31b16d2",
   "metadata": {},
   "source": [
    "### Small exercise: do a logistic regression of 2 features to 3 categories in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ff3450-72d1-497f-b64d-dfb205ae5b25",
   "metadata": {},
   "source": [
    "Uncomment and run the debugging code if it's helpful to do so.\n",
    "\n",
    "Remember: examine all your variables and look at each step! That's why we're using Python (and Jupyter) in the first place!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01b7b77-62d1-461c-86b1-5da163818b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = torch.tensor(features_np, dtype=torch.float32)\n",
    "targets = torch.tensor(targets_np, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfa438c-d158-49c0-9907-f23935069f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf61109-1f46-4e9b-bd22-add556384385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8f4df4-6a71-4a17-a610-2058e253078c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeInput(nn.Module):\n",
    "    def __init__(self, mean1, std1, mean2, std2):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"mean\", torch.tensor([mean1, mean2], dtype=torch.float32))\n",
    "        self.register_buffer(\"std\", torch.tensor([std1, std2], dtype=torch.float32))\n",
    "    def forward(self, x):\n",
    "        return (x - self.mean) / self.std\n",
    "\n",
    "normalize_input = NormalizeInput(\n",
    "    features_np[:, 0].mean(), features_np[:, 0].std(),\n",
    "    features_np[:, 1].mean(), features_np[:, 1].std(),\n",
    ")\n",
    "# normalize_input.forward(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb48e4f-4ffe-48ff-81b9-a6893200e466",
   "metadata": {},
   "source": [
    "The softmax function is defined as:\n",
    "\n",
    "$$ \\mbox{softmax}_i(x) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)} $$\n",
    "\n",
    "Consequentially,\n",
    "\n",
    "$$ \\sum_i \\mbox{softmax}_i(x) = 1 $$\n",
    "\n",
    "When we had only 2 categories, we could focus on only one of them and compute its sigmoid.\n",
    "\n",
    "With 3 categories, we use the softmax to ensure that the model's output probabilities add to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560f0a3c-42a3-42ad-a675-fe6217c254a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_without_softmax = nn.Sequential(\n",
    "    normalize_input,\n",
    "    nn.Linear(2, 3),\n",
    ")\n",
    "# model_without_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7794e7ab-1d2a-49fb-b7af-b05be54c4b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_without_softmax(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5447dfe5-b4a6-449e-9392-233e25ea64c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_softmax = nn.Sequential(\n",
    "    model_without_softmax,\n",
    "    nn.Softmax(dim=1),\n",
    ")\n",
    "# model_with_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366704b0-d3cc-499a-831d-d8615420cfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_with_softmax(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e63983b-e9a4-416c-a239-65552d2ce864",
   "metadata": {},
   "source": [
    "In the next cell, define a `loss_function`, an `optimizer`, and implement the fit.\n",
    "\n",
    "Use PyTorch's [nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html), which internally includes the softmax. Don't apply it twice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32a51c2-aab4-483c-9d48-d7fc96a22691",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0cf4a3e6-9850-4649-86db-f3991daf76d8",
   "metadata": {},
   "source": [
    "The next cell plots the fit, just like Scikit-Learn, using the model with the softmax (because we need to plot probabilities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a5c358-233b-414c-bd69-b576c280f59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# draw three scatter plots, each a different color\n",
    "ax.scatter(features_np[targets_np[:, 0] == 1, 0], features_np[targets_np[:, 0] == 1, 1])\n",
    "ax.scatter(features_np[targets_np[:, 1] == 1, 0], features_np[targets_np[:, 1] == 1, 1])\n",
    "ax.scatter(features_np[targets_np[:, 2] == 1, 0], features_np[targets_np[:, 2] == 1, 1])\n",
    "\n",
    "# compute the three probabilities for every 2D point in the background\n",
    "background_x, background_y = np.meshgrid(np.linspace(29, 62, 100), np.linspace(12, 23, 100))\n",
    "background_2d = np.column_stack([background_x.ravel(), background_y.ravel()])\n",
    "probabilities = model_with_softmax(torch.tensor(background_2d, dtype=torch.float32)).detach().numpy()\n",
    "\n",
    "# draw contour lines where the probabilities cross the 50% threshold\n",
    "ax.contour(background_x, background_y, probabilities[:, 0].reshape(background_x.shape), [0.5])\n",
    "ax.contour(background_x, background_y, probabilities[:, 1].reshape(background_x.shape), [0.5])\n",
    "ax.contour(background_x, background_y, probabilities[:, 2].reshape(background_x.shape), [0.5])\n",
    "\n",
    "ax.set_xlabel(\"Bill length (mm)\")\n",
    "ax.set_ylabel(\"Bill depth (mm)\")\n",
    "ax.axis([29, 62, 12, 23])\n",
    "\n",
    "ax.legend([\"Adelie\", \"Gentoo\", \"Chinstrap\"])\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe49568-91ca-4cbe-9706-9d9b1710d54f",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a6b135-7127-48ef-8b71-082761abcbb9",
   "metadata": {},
   "source": [
    "## Optimizers: learning rate, batches, and epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faf9fae-3465-4df5-be50-94a4d22944a0",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5a9ff5-0fb5-45e3-9f63-0e9637364ac8",
   "metadata": {},
   "source": [
    "## Feature selection and the \"kernel trick\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8056b5d5-1cd0-44e8-8429-fd08e90a1c05",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19b00de-3e2b-4961-90cc-d100665f0e52",
   "metadata": {},
   "source": [
    "## Under & overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de00e3ef-945f-4130-b70f-e1d363db635b",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cea7f5c-f2e6-42b8-bf62-241b110b5830",
   "metadata": {},
   "source": [
    "## Parameters versus hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faa3d01-536f-4bdc-8912-ed90106a8728",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806851fb-200c-478f-895c-de0bfbc47d6b",
   "metadata": {},
   "source": [
    "## Partitioning data into train-test-validate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415c3f35-9607-4569-96b4-3a595b434f5b",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f116d96-c26a-48d7-8c6e-2c0f593fc445",
   "metadata": {},
   "source": [
    "## Goodness of fit metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d821db71-30da-4bf0-9edb-c247cc1a0765",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f79eda-1416-4899-b607-cbf09b5509ea",
   "metadata": {},
   "source": [
    "## Regularization: L1, L2, dropout"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
